{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc4459d3",
   "metadata": {},
   "source": [
    "# L10c: Multiplicative Weights Update Algorithm and Zero-Sum Games\n",
    "In this lab, we'll use the multiplicative weights update algorithm to find approximate Nash equilibria in zero-sum games, in particular focusing on the classic Rock-Paper-Scissors game.\n",
    "\n",
    "> __Learning Objectives:__\n",
    ">\n",
    "> After completing this activity, students will be able to:\n",
    ">\n",
    "> * **Understand the Multiplicative Weights Algorithm:** We explore the theoretical foundations of the Multiplicative Weights Algorithm, including how it adapts strategies in zero-sum games by updating weights based on performance, leading to approximate Nash equilibria.\n",
    "> * **Implement the Multiplicative Weights Algorithm:** We build and execute the MWA for zero-sum games using Rock-Paper-Scissors as an example. The implementation shows how to simulate repeated play and compute average strategies over time.\n",
    "> * **Analyze convergence to Nash equilibrium:** We empirically check if the algorithm converges to an approximate Nash equilibrium by computing average strategies and verifying the ε-condition holds as the number of rounds increases.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6be8805",
   "metadata": {},
   "source": [
    "## Background: Multiplicative Weights Algorithm (MWA)\n",
    "The **Multiplicative Weights Algorithm (MWA)** is a simple yet robust online learning method that embodies a similar idea to the weighted majority algorithm, i.e., learning from expert advice. Here, the learning rate $\\eta$ plays a role analogous to $\\varepsilon$ in the Weighted Majority Algorithm, controlling adaptation speed. \n",
    "\n",
    "Let’s walk through the setup and sketch out the algorithm.\n",
    "\n",
    "### Problem Setting\n",
    "Suppose we are faced with a repeated decision-making task over rounds $t = 1, 2, \\ldots, T$. At each round, we have access to $N$ experts, each providing a recommendation or prediction. Our goal is to combine their advice adaptively in order to make strong decisions over time, even in adversarial or uncertain environments.\n",
    "\n",
    "* Let $\\mathbf{p}^{(t)} = \\{p_1^{(t)}, p_2^{(t)}, \\ldots, p_N^{(t)}\\}$ denote our belief distribution over experts at round $t$, updated iteratively based on their past performance.\n",
    "* We select an expert by sampling from this distribution—for example, using a Categorical distribution:\n",
    "  $i \\sim \\texttt{Categorical}(\\mathbf{p}^{(t)})$—and follow that expert’s recommendation.\n",
    "* After the decision is made, the environment (or adversary) reveals the true outcome. We then compute a cost vector $\\mathbf{m}^{(t)} = \\{m_1^{(t)}, \\dots, m_N^{(t)}\\}$, where $m_i^{(t)} \\in [-1, 1]$ denotes the cost incurred by expert $i$ at time $t$. A correct prediction receives a cost of $-1$, and an incorrect one receives a cost of $+1$.\n",
    "\n",
    "### Algorithm\n",
    "__Initialize__: Fix a learning rate $\\eta\\leq{1}/{2}$, for each expert initialize the weight $w_{i}^{(1)} = 1$.\n",
    "\n",
    "For $t=1,2,\\dots,T$:\n",
    "1. Chose expert $i$ with probability $p_{i}^{(t)} = w_{i}^{(t)}/\\sum_{j=1}^{N}w_{j}^{(t)}$. Ask expert $i$ what the outcome of the experiment should be, denote the experts answer to this as: $\\hat{y}_{i}^{(t)}$.\n",
    "2. The adversary (nature) reveals the true outcome $y_{t}$ of the experiment at time $t$. Compute the cost of the following expert $i$, denoted as $m_{i}^{(t)}$. \n",
    "    $$\n",
    "    m_i^{(t)} =\n",
    "    \\begin{cases}\n",
    "    -1 & \\text{if } \\hat{y}_i^{(t)} = y_t \\quad \\text{(correct)} \\\\\n",
    "    +1 & \\text{if } \\hat{y}_i^{(t)} \\neq y_t \\quad \\text{(incorrect)}\n",
    "    \\end{cases}\n",
    "   $$\n",
    "3. Update the weights of expert $i$ as (renormalize the weights to obtain the new probability distribution):\n",
    "$$\n",
    "\\begin{align*}\n",
    "w_{i}^{(t+1)} = w_{i}^{(t)}\\cdot\\left(1-\\eta\\cdot{m_{i}^{(t)}}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This is a super simple algorithm, with some very nice properties. The weights are updated multiplicatively based on the performance of each expert, hence the name Multiplicative Weights Algorithm. The learning rate $\\eta$ controls how aggressively the algorithm adapts to the experts' performance. And there is a theoretical guarantee that the algorithm will perform nearly as well as the best fixed expert in hindsight!\n",
    "\n",
    "By choosing $\\eta = \\sqrt{\\frac{\\ln N}{T}}$, this regret bound becomes __sublinear__:\n",
    "$$\n",
    "R(T) \\leq 2 \\sqrt{T \\ln N}\n",
    "$$\n",
    "This ensures that the algorithm's **average regret per round** vanishes as $T \\to \\infty$, meaning that MWA performs nearly as well as the best fixed expert in hindsight.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363ec521",
   "metadata": {},
   "source": [
    "## MWA Applied to zero-sum games\n",
    "Let's consider the application of the multiplicative weights update algorithm to zero-sum games. \n",
    "\n",
    "> In [a zero-sum game](https://en.wikipedia.org/wiki/Zero-sum_game), players have _opposing interests_, and the players' payoffs sum to zero: one's gain is the other's loss. The multiplicative-weights (MW) algorithm finds (approximate) Nash equilibria by down-weighting poorly performing actions over repeated play.\n",
    "\n",
    "Let's dig into some the details of the game:\n",
    "* **Game**: Consider a competitive setting with $k$ players. A game is called **zero-sum** if, for any outcome, the players' payoffs add to zero. The standard theory we use below focuses on the $k = 2$ case. Each player chooses an action $a \\in \\mathcal{A}$ from some finite action set $\\mathcal{A}$ with $|\\mathcal{A}| = N$. For the two-player case, we model payoffs with a matrix $\\mathbf{M} \\in \\mathbb{R}^{N \\times N}$ (for simplicity, assume both players have $N$ actions). If the row player chooses action $i$ and the column player chooses action $j$, then the row player's payoff is $m_{ij}$ and the column player's payoff is $-m_{ij}$. This is what we mean by __zero-sum__: whatever one player gains, the other loses.\n",
    "\n",
    "* **Goals**: The row player wants to **maximize** their payoff. The column player wants to **minimize** the row player's payoff. Let the row player randomize over rows using a mixed strategy $\\mathbf{p}$ (a probability distribution over the $N$ rows), and let the column player randomize over columns using a mixed strategy $\\mathbf{q}$ (a probability distribution over the $N$ columns). The expected payoff to the row player is $\\mathbf{p}^{\\top}\\mathbf{M}\\mathbf{q}$ and because the game is zero-sum, the expected payoff to the column player is $-\\mathbf{p}^{\\top}\\mathbf{M}\\mathbf{q}$. So both players care about the same scalar $\\mathbf{p}^{\\top}\\mathbf{M}\\mathbf{q}$, but they pull it in opposite directions.\n",
    "\n",
    "* **Nash Equilibrium**: A Nash equilibrium is a pair of (possibly mixed) strategies $(\\mathbf{p}^*, \\mathbf{q}^*)$ such that each player's strategy is a best response to the other's. In other words, given $\\mathbf{q}^*$, the row player cannot switch from $\\mathbf{p}^*$ to some other $\\mathbf{p}$ and improve their expected payoff, and given $\\mathbf{p}^*$, the column player cannot switch from $\\mathbf{q}^*$ to some other $\\mathbf{q}$ and further reduce the row player's expected payoff.\n",
    "\n",
    "In a two-player zero-sum game, every Nash equilibrium corresponds to a **minimax solution**. The minimax theorem guarantees that:\n",
    "$$\n",
    "\\max_{\\mathbf{p}} \\min_{\\mathbf{q}} \\mathbf{p}^{\\top}\\mathbf{M}\\mathbf{q} = \\min_{\\mathbf{q}} \\max_{\\mathbf{p}} \\mathbf{p}^{\\top}\\mathbf{M}\\mathbf{q} = v\n",
    "$$\n",
    "where $v$ is called the value of the game. At equilibrium, the row player's strategy $\\mathbf{p}^*$ guarantees at least $v$ no matter what the column player does, and the column player's strategy $\\mathbf{q}^*$ holds the row player to at most $v$ no matter what the row player does. That shared value $v$ is the Nash equilibrium payoff.\n",
    "  \n",
    "Finally, learning dynamics: if both players repeatedly play the game and update their mixed strategies using sublinear algorithms such as multiplicative weights, then the time-averaged strategies approach an $\\epsilon$-Nash equilibrium (equivalently, an $\\epsilon$-minimax solution), where $\\epsilon$ becomes small as regret becomes small.\n",
    "\n",
    "\n",
    "### Algorithm\n",
    "Let's outline a simple implementation of the multiplicative weights update algorithm for a two-player zero-sum game. Given a payoff matrix $\\mathbf{M}\\in\\mathbb{R}^{N\\times{N}}$, we want to find a _mixed strategy_, a probability distribution over actions, for the row player that minimizes expected loss.\n",
    "\n",
    "__Initialization:__ Given a payoff matrix $\\mathbf{M}\\in\\mathbb{R}^{N\\times{N}}$, where the payoffs (elements of $\\mathbf{M}$) are in the range $m_{ij}\\in[-1, 1]$. \n",
    "Initialize the weights $w_{i}^{(1)} \\gets 1$ for all actions $i\\in\\mathcal{A}$, where $\\mathcal{A} = \\{1,2,\\dots,N\\}$, and set the learning rate $\\eta\\in(0,1)$.\n",
    "\n",
    "> __Choosing T__: The number of rounds $T$ determines the accuracy of the approximate Nash equilibrium. To achieve an $\\epsilon$-Nash equilibrium, choose $T \\geq \\frac{\\ln N}{\\epsilon^2}$. For example, with $N=10$ actions and desired accuracy $\\epsilon=0.1$, we need $T \\geq \\frac{\\ln 10}{0.01} \\approx 230$ rounds.\n",
    "\n",
    "> __Choosing η__: The learning rate $\\eta$ controls the step size of weight updates. Common rules of thumb include:\n",
    "> - __Theory-based__: $\\eta = \\sqrt{\\frac{\\ln N}{T}}$ optimizes the convergence bound\n",
    "> - __Simple rule__: $\\eta = \\frac{1}{\\sqrt{T}}$ for practical applications  \n",
    "> - __Adaptive__: Start with $\\eta = 0.1$ and reduce by half if convergence stalls\n",
    "> - __Constraint__: Ensure $\\eta \\leq 1$ to prevent negative weights (since losses are bounded in $[-1,1]$)\n",
    "\n",
    "For each round $t=1,2,\\dots,T$ __do__:\n",
    "1. Compute the normalization factor: $\\Phi^{(t)} \\gets \\sum_{i=1}^{N}w_{i}^{(t)}$.\n",
    "1. __Row player__ computes its strategy: The _row player_ will choose an action with probability $\\mathbf{p}^{(t)} \\gets \\left\\{w_{i}^{(t)}/\\Phi^{(t)} \\mid i = 1,2,\\dots,N\\right\\}$. Let the row player action be $i^{\\star}$.\n",
    "2. __Column player__ computes its strategy: The _column player_ will choose action: $j\\gets \\text{arg}\\min_{j\\in\\mathcal{A}}\\left\\{\\mathbf{p}^{(t)\\top}\\mathbf{M}\\mathbf{e}_{j}\\right\\}$, so that $\\mathbf{q}^{(t)} \\gets \\mathbf{e}_{j}$, where $\\mathbf{e}_{j}$ is the $j$-th standard basis vector. The row player experiences loss vector $\\boldsymbol{\\ell}^{(t)} \\gets \\mathbf{L}\\mathbf{q}^{(t)}$, where $\\mathbf{L} = -\\mathbf{M}$ is the loss matrix.\n",
    "3. Update the weights: $w_i^{(t+1)} \\gets w_i^{(t)}\\;\\exp\\bigl(-\\eta\\,\\ell_i^{(t)}\\bigr)$ for all actions $i\\in\\mathcal{A}$ for the row player.\n",
    "\n",
    "### Convergence\n",
    "After $T$ rounds, define the average strategies:  \n",
    "$$\n",
    "\\bar p \\;=\\;\\frac{1}{T}\\sum_{t=1}^{T}p^{(t)}, \n",
    "\\quad\n",
    "\\bar q \\;=\\;\\frac{1}{T}\\sum_{t=1}^{T}q^{(t)}.\n",
    "$$\n",
    "Then $(\\bar p,\\bar q)$ is an $\\epsilon$-Nash equilibrium with\n",
    "$$\n",
    "  \\max_{q}\\,\\bar p^\\top M\\,q\n",
    "  \\;-\\;\\min_{p}\\,p^\\top M\\,\\bar q\n",
    "  \\;\\le\\;\\epsilon,\n",
    "  \\quad\n",
    "  \\epsilon = O\\Bigl(\\sqrt{\\tfrac{\\ln N}{T}}\\Bigr).\n",
    "$$\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d050a",
   "metadata": {},
   "source": [
    "## Example: Rock-Paper-Scissors\n",
    "Let's consider an example of a two-player zero-sum game: [Rock-Paper-Scissors](https://en.wikipedia.org/wiki/Rock_paper_scissors). In this game, each player _simultaneously_ chooses one of three possible actions: Rock, Paper, or Scissors. This game has three possible outcomes: win, loose or draw.\n",
    "> __Rules:__ A player who decides to play rock will beat another player who chooses scissors (`rock crushes scissors`), but will lose to one who has played paper (`paper covers rock`); a play of paper will lose to a play of scissors (`scissors cuts paper`). If both players choose the same shape, the game is a draw.\n",
    "\n",
    "The payoff matrix for this game is the `3` $\\times$ `3` matrix:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{M} = \\begin{pmatrix}\n",
    "0 & -1 & 1\\\\\n",
    "1 & 0 & -1\\\\\n",
    "-1 & 1 & 0\n",
    "\\end{pmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "where the rows correspond to the actions of the _row player_ and the columns, correspond to the actions of the _column player_. The payoff for the _row player_ is $m_{ij}$, and the payoff for the _column player_ is $-m_{ij}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1d3757a-b968-443d-a1a9-15520532d8dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: SystemError: opening file \"/Users/jdv27/Desktop/julia_work/CHEME-5800-Instances/Fall-2025/CHEME-5800-Labs-Fall-2025/labs/week-10/L10d/src/Files.jl\": No such file or directory\nin expression starting at /Users/jdv27/Desktop/julia_work/CHEME-5800-Instances/Fall-2025/CHEME-5800-Labs-Fall-2025/labs/week-10/L10d/Include-student.jl:28",
     "output_type": "error",
     "traceback": [
      "LoadError: SystemError: opening file \"/Users/jdv27/Desktop/julia_work/CHEME-5800-Instances/Fall-2025/CHEME-5800-Labs-Fall-2025/labs/week-10/L10d/src/Files.jl\": No such file or directory\n",
      "in expression starting at /Users/jdv27/Desktop/julia_work/CHEME-5800-Instances/Fall-2025/CHEME-5800-Labs-Fall-2025/labs/week-10/L10d/Include-student.jl:28\n",
      "\n",
      "Stacktrace:\n",
      " [1] include(mapexpr::Function, mod::Module, _path::String)\n",
      "   @ Base ./Base.jl:307\n",
      " [2] top-level scope\n",
      "   @ ~/Desktop/julia_work/CHEME-5800-Instances/Fall-2025/CHEME-5800-Labs-Fall-2025/labs/week-10/L10d/Include-student.jl:28"
     ]
    }
   ],
   "source": [
    "include(\"Include-student.jl\"); # load my codes, packages, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67424f3-a604-4cee-a8b4-f6f8e7d95d38",
   "metadata": {},
   "source": [
    "__Build a model__. Let's construct an instance of [the `MyTwoPersonZeroSumGameModel` type](src/Types.jl) using [a custom `build(...)` method](src/Factory.jl). The model holds information associated with the game. We store the game model in the `model::MyTwoPersonZeroSumGameModel` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8148173-7992-47d3-97d8-996698b79266",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = let\n",
    "\n",
    "    # setup \n",
    "    M = [0 -1 1; 1 0 -1 ; -1 1 0]; # rock paper scissors payoff matrix\n",
    "    T = 2000; # number of rounds we play the game\n",
    "    n = 3; # number of actions\n",
    "    η = sqrt(log(n)/T); # learning rate\n",
    "\n",
    "    # build a model -\n",
    "    model = build(MyTwoPersonZeroSumGameModel, (\n",
    "        ϵ = η, # learning rate\n",
    "        n = n, # number of actions\n",
    "        T = T, # number of rounds we play the game\n",
    "        payoffmatrix = M, # payoff matrix\n",
    "    ));\n",
    "\n",
    "    model; # return the \n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff21629e-0abe-4ec2-98d5-1605be30f8ae",
   "metadata": {},
   "source": [
    "__Play the game__. Next, we play the game. We pass the `model::MyTwoPersonZeroSumGameModel` instance into [the `play(...)` method](src/Online.jl) as the only argument. This method returns the raw game output, where each row is a game instance (round), each column is a player action, and the weights matrix.\n",
    "* The `rps_sims::Array{Int64,2}` array holds the outcome of each game encoded as 1 = rock, 2 = paper and 3 = scissors. The first column is the _row player_, while the second is the _column player_.\n",
    "* The `weights::Array{Float64,2}` holds the _row player_ distribution for each instance of the game.\n",
    "\n",
    "Let's play the game for $T$ rounds and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3020475-e444-4ba1-aa4f-1eed55fa77f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(rps_sim, weights) = play(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f03aeaa",
   "metadata": {},
   "source": [
    "What's in the rps_sims and weights variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fc50bb1-ddcb-4851-8621-c8538c96a75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000×2 Matrix{Int64}:\n",
       " 1  3\n",
       " 3  3\n",
       " 3  3\n",
       " 1  3\n",
       " 1  3\n",
       " 1  3\n",
       " 1  3\n",
       " 3  3\n",
       " 2  3\n",
       " 1  3\n",
       " ⋮  \n",
       " 1  3\n",
       " 1  3\n",
       " 1  3\n",
       " 1  3\n",
       " 1  3\n",
       " 1  3\n",
       " 1  3\n",
       " 1  3\n",
       " 1  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rps_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedb0260-4742-4790-ae5b-10684e837cf0",
   "metadata": {},
   "source": [
    "__Games outcome table__. `Unhide` the code block below to see how we constructed the game table [using the `pretty_tables(...)` method exported by the `PrettyTables.jl` package](https://github.com/ronisbr/PrettyTables.jl).\n",
    "\n",
    "> __Summary:__ Each row of the table displays the game's outcome. The first column shows the action of the _row player_, while the second column shows the (near) optimal action of the _column player_, given the action of the _row player_.\n",
    "\n",
    "So what do we see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58895868-95c5-41df-923e-4a0d79fce793",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------- ------------ ------------\n",
      " \u001b[1m  game \u001b[0m \u001b[1m row_player \u001b[0m \u001b[1m col_player \u001b[0m\n",
      " \u001b[90m Int64 \u001b[0m \u001b[90m     String \u001b[0m \u001b[90m     String \u001b[0m\n",
      " ------- ------------ ------------\n",
      "      1         rock     scissors\n",
      "      2     scissors     scissors\n",
      "      3     scissors     scissors\n",
      "      4         rock     scissors\n",
      "      5         rock     scissors\n",
      "      6         rock     scissors\n",
      "      7         rock     scissors\n",
      "      8     scissors     scissors\n",
      "      9        paper     scissors\n",
      "     10         rock     scissors\n",
      "     11     scissors     scissors\n",
      "     12     scissors     scissors\n",
      "     13     scissors     scissors\n",
      "     14         rock     scissors\n",
      "     15         rock     scissors\n",
      "      ⋮            ⋮            ⋮\n",
      " ------- ------------ ------------\n",
      "\u001b[36m                  1985 rows omitted\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    # initialize -\n",
    "    T = model.T;\n",
    "    moves = Dict{Int, String}(1 => \"rock\", 2=> \"paper\", 3=>\"scissors\"); # setup moves map\n",
    "    df = DataFrame();\n",
    "\n",
    "    # build rounds table -\n",
    "    for t ∈ 1:T\n",
    "        row_df = (\n",
    "            game = t,\n",
    "            row_player = rps_sim[t,1] |> i-> moves[i],\n",
    "            col_player = rps_sim[t,2] |> i-> moves[i],\n",
    "        )\n",
    "        push!(df, row_df);\n",
    "    end\n",
    "    \n",
    "    # build a table -\n",
    "    pretty_table(\n",
    "         df;\n",
    "         backend = :text,\n",
    "         table_format = TextTableFormat(borders = text_table_borders__compact)\n",
    "    );\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b19f09b",
   "metadata": {},
   "source": [
    "### Check for convergence\n",
    "To check if the algorithm has converged to an approximate Nash equilibrium, we need to compute the average strategies:\n",
    "\n",
    "> **Average row player strategy $\\bar{p}$**: The average of the probability distributions $\\mathbf{p}^{(t)}$ over all rounds. We'll compute that from looking at the weights matrix.\n",
    "> \n",
    "> **Average column player strategy $\\bar{q}$**: The average of the one-hot vectors $\\mathbf{q}^{(t)}$ (i.e., the empirical frequency of each action chosen by the column player).\n",
    "\n",
    "From the weights matrix, we can compute $\\mathbf{p}^{(t)} = \\frac{\\text{weights}[t, :]}{\\sum \\text{weights}[t, :]}$ for each $t$. For $\\mathbf{q}^{(t)}$, since the column player plays deterministically, $\\mathbf{q}^{(t)}$ is a one-hot vector based on the action in $\\text{rps\\_sim}[t, 2]$.\n",
    "\n",
    "Let's compute these averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0b28ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average row player strategy (p̄): [0.9846274315653998, 0.0006138829367003676, 0.014758685497901778]\n",
      "Average column player strategy (q̄): [0.0, 0.0, 1.0]\n",
      "Approximate ε (max_q p̄^T M q - min_p p^T M q̄): 1.9840135486286994\n",
      "[0.9846274315653998, 0.0006138829367003676, 0.014758685497901778]\n",
      "Average column player strategy (q̄): [0.0, 0.0, 1.0]\n",
      "Approximate ε (max_q p̄^T M q - min_p p^T M q̄): 1.9840135486286994\n"
     ]
    }
   ],
   "source": [
    "eps_approx = let\n",
    "\n",
    "\n",
    "    # initialize -\n",
    "    T = model.T;\n",
    "    n = model.n;\n",
    "    \n",
    "    # Average row player strategy p̄ (compute from the weights)\n",
    "    p_avg = zeros(Float64, n);\n",
    "    for t in 1:T\n",
    "        Φ = sum(weights[t, :]);\n",
    "        p_t = weights[t, :] / Φ;\n",
    "        p_avg += p_t; # does element-wise addition\n",
    "    end\n",
    "    p_avg = (1/T)*p_avg;\n",
    "\n",
    "    # Average column player strategy q̄ (empirical frequencies)\n",
    "    q_avg = zeros(Float64, n);\n",
    "    for t in 1:T\n",
    "        action = rps_sim[t, 2];\n",
    "        q_avg[action] += 1.0;\n",
    "    end\n",
    "    q_avg = (1/T)*q_avg;\n",
    "    \n",
    "    # Let's print out the average strategies\n",
    "    println(\"Average row player strategy (p̄): \", p_avg);\n",
    "    println(\"Average column player strategy (q̄): \", q_avg);\n",
    "\n",
    "    # Check for convergence\n",
    "    M = model.payoffmatrix;\n",
    "    max_part = maximum(p_avg' * M); # max_q p̄^T M q = max_j (p̄^T M)_j\n",
    "    min_part = minimum(M * q_avg);  # min_p p^T M q̄ = min_i (M q̄)_i\n",
    "    ε_approx = max_part - min_part;\n",
    "    println(\"Approximate ε (max_q p̄^T M q - min_p p^T M q̄): \", ε_approx);\n",
    "    ε_approx;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "074ffcf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023437281078104066"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.ϵ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a665d8",
   "metadata": {},
   "source": [
    "Ok, so if this did not convertion [the `@assert` statement](https://docs.julialang.org/en/v1/base/base/#Base.@assert) will blow up. What do we see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50982fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(x, y) = (-2, 0)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "AssertionError: isapprox(x, y)",
     "output_type": "error",
     "traceback": [
      "AssertionError: isapprox(x, y)\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Desktop/julia_work/CHEME-5800-Instances/Fall-2025/CHEME-5800-Labs-Fall-2025/labs/week-10/L10d/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X31sZmlsZQ==.jl:8"
     ]
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    x = floor(Int, log10(model.ϵ));\n",
    "    y = floor(Int, log10(eps_approx));\n",
    "\n",
    "    @show x, y\n",
    "\n",
    "    @assert isapprox(x,y)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bc0259",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this lab, we implemented the Multiplicative Weights Algorithm for zero-sum games, using Rock-Paper-Scissors to demonstrate how it finds approximate Nash equilibria through repeated play.\n",
    "\n",
    "> __Key Takeaways:__\n",
    ">\n",
    "> * **Adaptive strategy updates:** The Multiplicative Weights Algorithm adjusts player strategies by updating weights based on payoffs, down-weighting poorly performing actions to encourage mixing.\n",
    "> * **Convergence to equilibrium:** Over many rounds, the average strategies approach an approximate Nash equilibrium, where neither player can gain by unilaterally changing strategy.\n",
    "> * **Application to zero-sum games:** The algorithm handles competitive settings like Rock-Paper-Scissors, converging to uniform mixing (equal probability for each action) as the optimal response to an adaptive opponent.\n",
    "\n",
    "The Multiplicative Weights Algorithm effectively finds balanced strategies in zero-sum games without prior knowledge of the opponent's moves.\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd44a21",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.1",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
