{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a1dc826-afaa-46c8-8e6b-dfc8187e457e",
   "metadata": {},
   "source": [
    "# L9d: Estimating a Logistic Regression Binary Classifier using Gradient Descent\n",
    "In this lab, we will implement a logistic regression binary classifier using gradient descent. Logistic regression is a statistical method for predicting binary outcomes based on one or more predictor variables. It is widely used in various fields, including machine learning, medical research, and social sciences.\n",
    "\n",
    "> __Learning Objectives:__\n",
    ">\n",
    "> After completing this activity, students will be able to: \n",
    "> * **Derive logistic regression from statistical mechanics:** We develop the logistic regression model by viewing binary class labels as states in a Boltzmann distribution and derive the cross-entropy loss function. This approach connects statistical mechanics concepts with machine learning classification problems.\n",
    "> * **Implement gradient descent optimization:** We use gradient descent to minimize the cross-entropy loss function and find optimal classifier parameters. The implementation uses finite difference approximations to compute gradients and iteratively updates parameters until convergence.\n",
    "> * **Train and evaluate a binary classifier:** We train a logistic regression model on the banknote authentication dataset and evaluate its performance using a confusion matrix. The model learns to distinguish genuine from forged banknotes based on wavelet-transformed image features.\n",
    "\n",
    "This is going to be cool, so let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe866a08",
   "metadata": {},
   "source": [
    "## Background: Cross-entropy loss\n",
    "Suppose we view our two–class labels $y\\in\\{-1,1\\}$ as _states_ in a Boltzmann distribution conditioned on the input $\\hat{\\mathbf{x}}\\in\\mathbb{R}^{m+1}$ (the original feature vector with a `1` as the last element to account for a bias). Then for any state $y$ with energy $E(y,\\hat{\\mathbf{x}})$ at (unit) temperature, the conditional probability of observing the label $y\\in\\left\\{-1,+1\\right\\}$ given the feature vector $\\hat{\\mathbf{x}}$ can be represented as\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(y\\mid \\hat{\\mathbf{x}})\n",
    "=\\frac{\\exp\\bigl(-E(y,\\hat{\\mathbf{x}})\\bigr)}\n",
    "      {\\underbrace{\\sum_{y' \\in\\{-1,1\\}} \\exp\\bigl(-E(y',\\hat{\\mathbf{x}})\\bigr)}_{Z(\\hat{\\mathbf{x}})}}.\n",
    "\\end{align*}\n",
    "$$\n",
    "For the energy function, we can use a linear model of the form:\n",
    "$$\n",
    "\\begin{align*}\n",
    "E(y,\\hat{\\mathbf{x}})\\;=\\;-\\,y\\;\\bigl(\\hat{\\mathbf{x}}^{\\top}\\theta \\bigr).\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\theta\\in\\mathbb{R}^{p}$ is a vector of __unknown__ parameters (weights plus bias) that we want to learn. When $y=+1$, the energy $E(1,\\hat{\\mathbf{x}})=-\\hat{\\mathbf{x}}^{\\top}\\theta$ is *lower* (more probable) if $\\hat{\\mathbf{x}}^{\\top}\\theta$ is large. On the other hand, when $y=-1$, the energy $E(-1,\\hat{\\mathbf{x}})=+\\hat{\\mathbf{x}}^{\\top}\\theta$, so $y=-1$ is favored when $\\hat{\\mathbf{x}}^{\\top}\\theta$ is very negative.\n",
    "\n",
    "Let's substitute the energy function into the conditional probability expression and do some algebra:\n",
    "$$\n",
    "\\begin{align*}\n",
    "P_{\\theta}(y\\mid \\hat{\\mathbf{x}})\n",
    "& =\\frac{\\exp\\bigl(-E(y,\\hat{\\mathbf{x}})\\bigr)}\n",
    "      {\\underbrace{\\sum_{y' \\in\\{-1,1\\}} \\exp\\bigl(-E(y',\\hat{\\mathbf{x}})\\bigr)}_{Z(\\hat{\\mathbf{x}})}}\\\\\n",
    "&=\\frac{\\exp\\bigl(y\\left(\\hat{\\mathbf{x}}^{\\top}\\theta\\right)\\bigr)}\n",
    "      {\\exp\\bigl(\\hat{\\mathbf{x}}^{\\top}\\theta\\bigr) + \\exp\\bigl(-\\hat{\\mathbf{x}}^{\\top}\\theta\\bigr)}\\quad\\Longrightarrow\\;{\\text{substituting } z = \\hat{\\mathbf{x}}^{\\top}\\theta}\\\\\n",
    "& = \\frac{\\exp\\bigl(yz\\bigr)}\n",
    "      {\\exp\\bigl(z\\bigr) + \\exp\\bigl(-z\\bigr)}\\quad\\Longrightarrow\\;{\\text{factor out}\\; \\exp(yz)\\;\\text{from denominator}}\\\\\n",
    "& = \\frac{\\exp\\bigl(yz\\bigr)}\n",
    "      {\\exp\\bigl(yz\\bigr)\\left(\\exp\\bigl((1-y)z\\bigr) + \\exp\\bigl(-(1+y)z\\bigr)\\right)}\\quad\\Longrightarrow\\;\\text{cancel}\\;\\exp(yz)\\\\\n",
    "& = \\frac{1}\n",
    "      {\\exp\\bigl((1-y)z\\bigr) + \\exp\\bigl(-(1+y)z\\bigr)}\\quad\\blacksquare\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This expression is the probability of observing the label $y$ given the feature vector $\\hat{\\mathbf{x}}$ and the parameters $\\theta$. Let's look at the case when $y=+1$ and $y=-1$:\n",
    "\n",
    "> __Cases:__\n",
    ">\n",
    "> When $y=+1$, we have:\n",
    "> $$\n",
    "\\begin{align*}\n",
    "P_{\\theta}(y = +1\\mid \\hat{\\mathbf{x}})\n",
    "& = \\frac{1}\n",
    "      {\\exp\\bigl(0\\bigr) + \\exp\\bigl(-2\\left(\\hat{\\mathbf{x}}^{\\top}\\theta\\right)\\bigr)}\\\\\n",
    "& = \\frac{1}\n",
    "      {1 + \\exp\\bigl(-2\\left(\\hat{\\mathbf{x}}^{\\top}\\theta\\right)\\bigr)}\\quad\\blacksquare\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "> \n",
    "> When $y=-1$, we have:\n",
    "> $$\\begin{align*}\n",
    "P_{\\theta}(y = -1\\mid \\hat{\\mathbf{x}})\n",
    "& = \\frac{1}\n",
    "      {\\exp\\bigl(2\\left(\\hat{\\mathbf{x}}^{\\top}\\theta\\right)\\bigr) + \\exp\\bigl(0\\bigr)}\\\\\n",
    "& = \\frac{1}\n",
    "      {1+\\exp\\bigl(2\\left(\\hat{\\mathbf{x}}^{\\top}\\theta\\right)\\bigr)}\\quad\\blacksquare\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "> Putting this all together, we can write the conditional probability of observing the label $y$ given the feature vector $\\hat{\\mathbf{x}}$ and the parameters $\\theta$ as:\n",
    "> $$\\begin{align*}\n",
    "P_{\\theta}(y\\mid \\hat{\\mathbf{x}}) & = \\frac{1}{1+\\exp\\bigl(-2y\\left(\\hat{\\mathbf{x}}^{\\top}\\theta\\right)\\bigr)}\\quad\\Longrightarrow\\;\\text{Logistic function!}\\\\\n",
    "& = \\sigma\\bigl(2y\\left(\\hat{\\mathbf{x}}^{\\top}\\theta\\right)\\bigr)\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "### Parameter Estimation\n",
    "Of course, we want to learn the parameters $\\theta$ so that we maximize the log likelihood (or minimize the negative log-likelihood) of the observed labels given the feature vectors. The likelihood function is given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta) & = \\prod_{i=1}^{n} P_{\\theta}(y_{i}\\mid \\hat{\\mathbf{x}}_{i})\\\\\n",
    "& = \\prod_{i=1}^{n} \\frac{1}{1+\\exp\\bigl(-2y_{i}\\,\\left(\\hat{\\mathbf{x}}^{\\top}_{i}\\theta\\right)\\bigr)}\\quad\\Longrightarrow\\;\\text{Product is $\\textbf{hard}$ to optimize! Take the $\\log$}\\\\\n",
    "\\log\\mathcal{L}(\\theta) & = -\\sum_{i=1}^n \\log\\!\\bigl(1+\\exp\\bigl(-2y_i\\,\\left(\\hat{\\mathbf{x}}^{\\top}_{i}\\theta\\right)\\bigr)\\bigr)\\\\\n",
    "\\end{align*}\n",
    "$$  \n",
    "\n",
    "We can use gradient descent to minimize the negative log-likelihood (also known as the cross-entropy loss function):\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{align*}\n",
    "J(\\theta) & = -\\log\\mathcal{L}(\\theta)\\\\\n",
    "& = \\sum_{i=1}^n \\log\\!\\bigl(1+\\exp\\bigl(-2y_i\\,\\left(\\hat{\\mathbf{x}}^{\\top}_{i}\\theta\\right)\\bigr)\\bigr)\\quad\\blacksquare\\\\\n",
    "\\end{align*}}\n",
    "$$      \n",
    "This will give us the optimal parameters $\\theta$ for our logistic regression model:\n",
    "$$\n",
    "\\hat{\\theta} = \\arg\\min_{\\theta} J(\\theta)\n",
    "$$\n",
    "Ok, let's give this a try.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acf81bb",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "First, we set up the computational environment by including the `Include.jl` file and loading any needed resources.\n",
    "\n",
    "> The [`include(...)` command](https://docs.julialang.org/en/v1/base/base/#include) evaluates the contents of the input source file, `Include.jl`, in the notebook's global scope. The `Include.jl` file sets paths, loads required external packages, etc. For additional information on functions and types used in this material, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/). \n",
    "\n",
    "Let's set up our code environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aa04d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(joinpath(@__DIR__, \"Include-banknote-solution.jl\")); # include the Include.jl file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff647f88",
   "metadata": {},
   "source": [
    "In addition to standard Julia libraries, we'll also use [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl). Check out [the documentation](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/) for more information on the functions, types, and data used in this material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f243bb",
   "metadata": {},
   "source": [
    "### Data\n",
    "The dataset we will explore is the [banknote authentication dataset from the UCI archive](https://archive.ics.uci.edu/dataset/267/banknote+authentication). This dataset has `1372` instances with 4 continuous features and an integer $\\{-1,1\\}$ class variable. \n",
    "\n",
    "> __Description of the dataset__ \n",
    "> \n",
    "> * Data were extracted from images taken from genuine and forged banknote-like specimens. An industrial camera, usually used for print inspection, was used for digitization. The final images have 400x400 pixels. Due to the object lens and distance to the investigated object, gray-scale pictures with a resolution of about 660 dpi were obtained. Wavelet Transform tools were used to extract features from images.\n",
    "> * __Features__: The data has four continuous features from each image: `variance` of the wavelet transformed image, `skewness` of the wavelet transformed image, `kurtosis` of the wavelet transformed image, and the `entropy` of the wavelet transformed image. The class is $\\{-1,1\\}$ where a class value of `-1` indicates genuine and `1` indicates forged.\n",
    "\n",
    "We've included this dataset in [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl) and have provided [the `MyBanknoteAuthenticationDataset(...)` helper function](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/data/#VLDataScienceMachineLearningPackage.MyBanknoteAuthenticationDataset) for easy access. \n",
    "\n",
    "This method returns the data in [a `DataFrame` instance](https://github.com/JuliaData/DataFrames.jl), which we'll save in the `df_banknote` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20517586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>1372×5 DataFrame</span></div><div style = \"float: right; font-style: italic;\"><span>1347 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"columnLabelRow\"><th class = \"stubheadLabel\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">variance</th><th style = \"text-align: left;\">skewness</th><th style = \"text-align: left;\">curtosis</th><th style = \"text-align: left;\">entropy</th><th style = \"text-align: left;\">class</th></tr><tr class = \"columnLabelRow\"><th class = \"stubheadLabel\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th></tr></thead><tbody><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">3.6216</td><td style = \"text-align: right;\">8.6661</td><td style = \"text-align: right;\">-2.8073</td><td style = \"text-align: right;\">-0.44699</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">4.5459</td><td style = \"text-align: right;\">8.1674</td><td style = \"text-align: right;\">-2.4586</td><td style = \"text-align: right;\">-1.4621</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">3.866</td><td style = \"text-align: right;\">-2.6383</td><td style = \"text-align: right;\">1.9242</td><td style = \"text-align: right;\">0.10645</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">3.4566</td><td style = \"text-align: right;\">9.5228</td><td style = \"text-align: right;\">-4.0112</td><td style = \"text-align: right;\">-3.5944</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">0.32924</td><td style = \"text-align: right;\">-4.4552</td><td style = \"text-align: right;\">4.5718</td><td style = \"text-align: right;\">-0.9888</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">4.3684</td><td style = \"text-align: right;\">9.6718</td><td style = \"text-align: right;\">-3.9606</td><td style = \"text-align: right;\">-3.1625</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: right;\">3.5912</td><td style = \"text-align: right;\">3.0129</td><td style = \"text-align: right;\">0.72888</td><td style = \"text-align: right;\">0.56421</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: right;\">2.0922</td><td style = \"text-align: right;\">-6.81</td><td style = \"text-align: right;\">8.4636</td><td style = \"text-align: right;\">-0.60216</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: right;\">3.2032</td><td style = \"text-align: right;\">5.7588</td><td style = \"text-align: right;\">-0.75345</td><td style = \"text-align: right;\">-0.61251</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: right;\">1.5356</td><td style = \"text-align: right;\">9.1772</td><td style = \"text-align: right;\">-2.2718</td><td style = \"text-align: right;\">-0.73535</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: right;\">1.2247</td><td style = \"text-align: right;\">8.7779</td><td style = \"text-align: right;\">-2.2135</td><td style = \"text-align: right;\">-0.80647</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: right;\">3.9899</td><td style = \"text-align: right;\">-2.7066</td><td style = \"text-align: right;\">2.3946</td><td style = \"text-align: right;\">0.86291</td><td style = \"text-align: right;\">-1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: right;\">1.8993</td><td style = \"text-align: right;\">7.6625</td><td style = \"text-align: right;\">0.15394</td><td style = \"text-align: right;\">-3.1108</td><td style = \"text-align: right;\">-1</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1361</td><td style = \"text-align: right;\">-0.24745</td><td style = \"text-align: right;\">1.9368</td><td style = \"text-align: right;\">-2.4697</td><td style = \"text-align: right;\">-0.80518</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1362</td><td style = \"text-align: right;\">-1.5732</td><td style = \"text-align: right;\">1.0636</td><td style = \"text-align: right;\">-0.71232</td><td style = \"text-align: right;\">-0.8388</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1363</td><td style = \"text-align: right;\">-2.1668</td><td style = \"text-align: right;\">1.5933</td><td style = \"text-align: right;\">0.045122</td><td style = \"text-align: right;\">-1.678</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1364</td><td style = \"text-align: right;\">-1.1667</td><td style = \"text-align: right;\">-1.4237</td><td style = \"text-align: right;\">2.9241</td><td style = \"text-align: right;\">0.66119</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1365</td><td style = \"text-align: right;\">-2.8391</td><td style = \"text-align: right;\">-6.63</td><td style = \"text-align: right;\">10.4849</td><td style = \"text-align: right;\">-0.42113</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1366</td><td style = \"text-align: right;\">-4.5046</td><td style = \"text-align: right;\">-5.8126</td><td style = \"text-align: right;\">10.8867</td><td style = \"text-align: right;\">-0.52846</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1367</td><td style = \"text-align: right;\">-2.41</td><td style = \"text-align: right;\">3.7433</td><td style = \"text-align: right;\">-0.40215</td><td style = \"text-align: right;\">-1.2953</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1368</td><td style = \"text-align: right;\">0.40614</td><td style = \"text-align: right;\">1.3492</td><td style = \"text-align: right;\">-1.4501</td><td style = \"text-align: right;\">-0.55949</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1369</td><td style = \"text-align: right;\">-1.3887</td><td style = \"text-align: right;\">-4.8773</td><td style = \"text-align: right;\">6.4774</td><td style = \"text-align: right;\">0.34179</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1370</td><td style = \"text-align: right;\">-3.7503</td><td style = \"text-align: right;\">-13.4586</td><td style = \"text-align: right;\">17.5932</td><td style = \"text-align: right;\">-2.7771</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1371</td><td style = \"text-align: right;\">-3.5637</td><td style = \"text-align: right;\">-8.3827</td><td style = \"text-align: right;\">12.393</td><td style = \"text-align: right;\">-1.2823</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1372</td><td style = \"text-align: right;\">-2.5419</td><td style = \"text-align: right;\">-0.65804</td><td style = \"text-align: right;\">2.6842</td><td style = \"text-align: right;\">1.1952</td><td style = \"text-align: right;\">1</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& variance & skewness & curtosis & entropy & class\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & 3.6216 & 8.6661 & -2.8073 & -0.44699 & -1 \\\\\n",
       "\t2 & 4.5459 & 8.1674 & -2.4586 & -1.4621 & -1 \\\\\n",
       "\t3 & 3.866 & -2.6383 & 1.9242 & 0.10645 & -1 \\\\\n",
       "\t4 & 3.4566 & 9.5228 & -4.0112 & -3.5944 & -1 \\\\\n",
       "\t5 & 0.32924 & -4.4552 & 4.5718 & -0.9888 & -1 \\\\\n",
       "\t6 & 4.3684 & 9.6718 & -3.9606 & -3.1625 & -1 \\\\\n",
       "\t7 & 3.5912 & 3.0129 & 0.72888 & 0.56421 & -1 \\\\\n",
       "\t8 & 2.0922 & -6.81 & 8.4636 & -0.60216 & -1 \\\\\n",
       "\t9 & 3.2032 & 5.7588 & -0.75345 & -0.61251 & -1 \\\\\n",
       "\t10 & 1.5356 & 9.1772 & -2.2718 & -0.73535 & -1 \\\\\n",
       "\t11 & 1.2247 & 8.7779 & -2.2135 & -0.80647 & -1 \\\\\n",
       "\t12 & 3.9899 & -2.7066 & 2.3946 & 0.86291 & -1 \\\\\n",
       "\t13 & 1.8993 & 7.6625 & 0.15394 & -3.1108 & -1 \\\\\n",
       "\t14 & -1.5768 & 10.843 & 2.5462 & -2.9362 & -1 \\\\\n",
       "\t15 & 3.404 & 8.7261 & -2.9915 & -0.57242 & -1 \\\\\n",
       "\t16 & 4.6765 & -3.3895 & 3.4896 & 1.4771 & -1 \\\\\n",
       "\t17 & 2.6719 & 3.0646 & 0.37158 & 0.58619 & -1 \\\\\n",
       "\t18 & 0.80355 & 2.8473 & 4.3439 & 0.6017 & -1 \\\\\n",
       "\t19 & 1.4479 & -4.8794 & 8.3428 & -2.1086 & -1 \\\\\n",
       "\t20 & 5.2423 & 11.0272 & -4.353 & -4.1013 & -1 \\\\\n",
       "\t21 & 5.7867 & 7.8902 & -2.6196 & -0.48708 & -1 \\\\\n",
       "\t22 & 0.3292 & -4.4552 & 4.5718 & -0.9888 & -1 \\\\\n",
       "\t23 & 3.9362 & 10.1622 & -3.8235 & -4.0172 & -1 \\\\\n",
       "\t24 & 0.93584 & 8.8855 & -1.6831 & -1.6599 & -1 \\\\\n",
       "\t25 & 4.4338 & 9.887 & -4.6795 & -3.7483 & -1 \\\\\n",
       "\t26 & 0.7057 & -5.4981 & 8.3368 & -2.8715 & -1 \\\\\n",
       "\t27 & 1.1432 & -3.7413 & 5.5777 & -0.63578 & -1 \\\\\n",
       "\t28 & -0.38214 & 8.3909 & 2.1624 & -3.7405 & -1 \\\\\n",
       "\t29 & 6.5633 & 9.8187 & -4.4113 & -3.2258 & -1 \\\\\n",
       "\t30 & 4.8906 & -3.3584 & 3.4202 & 1.0905 & -1 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m1372×5 DataFrame\u001b[0m\n",
       "\u001b[1m  Row \u001b[0m│\u001b[1m variance \u001b[0m\u001b[1m skewness  \u001b[0m\u001b[1m curtosis  \u001b[0m\u001b[1m entropy  \u001b[0m\u001b[1m class \u001b[0m\n",
       "\u001b[1m      \u001b[0m│\u001b[90m Float64  \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Int64 \u001b[0m\n",
       "──────┼─────────────────────────────────────────────────\n",
       "    1 │  3.6216     8.6661   -2.8073    -0.44699     -1\n",
       "    2 │  4.5459     8.1674   -2.4586    -1.4621      -1\n",
       "    3 │  3.866     -2.6383    1.9242     0.10645     -1\n",
       "    4 │  3.4566     9.5228   -4.0112    -3.5944      -1\n",
       "    5 │  0.32924   -4.4552    4.5718    -0.9888      -1\n",
       "    6 │  4.3684     9.6718   -3.9606    -3.1625      -1\n",
       "    7 │  3.5912     3.0129    0.72888    0.56421     -1\n",
       "    8 │  2.0922    -6.81      8.4636    -0.60216     -1\n",
       "    9 │  3.2032     5.7588   -0.75345   -0.61251     -1\n",
       "   10 │  1.5356     9.1772   -2.2718    -0.73535     -1\n",
       "   11 │  1.2247     8.7779   -2.2135    -0.80647     -1\n",
       "  ⋮   │    ⋮          ⋮          ⋮         ⋮        ⋮\n",
       " 1363 │ -2.1668     1.5933    0.045122  -1.678        1\n",
       " 1364 │ -1.1667    -1.4237    2.9241     0.66119      1\n",
       " 1365 │ -2.8391    -6.63     10.4849    -0.42113      1\n",
       " 1366 │ -4.5046    -5.8126   10.8867    -0.52846      1\n",
       " 1367 │ -2.41       3.7433   -0.40215   -1.2953       1\n",
       " 1368 │  0.40614    1.3492   -1.4501    -0.55949      1\n",
       " 1369 │ -1.3887    -4.8773    6.4774     0.34179      1\n",
       " 1370 │ -3.7503   -13.4586   17.5932    -2.7771       1\n",
       " 1371 │ -3.5637    -8.3827   12.393     -1.2823       1\n",
       " 1372 │ -2.5419    -0.65804   2.6842     1.1952       1\n",
       "\u001b[36m                                       1351 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_banknote =  MyBanknoteAuthenticationDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97bbb7",
   "metadata": {},
   "source": [
    "Now let's split the dataset into the system input matrix $\\mathbf{X}$ (independent variables, characteristics of the banknote) and the output vector $\\mathbf{y}$ (dependent variable, the banknote class).\n",
    "\n",
    "The input matrix $\\mathbf{X}$ will contain all the columns except for the `class` column (the output variable). The output vector $\\mathbf{y}$ will contain only the `class` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3adbd92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Matrix(df_banknote[:, Not(:class)]); # data matrix: select all the columns *except* class\n",
    "y = Vector(df_banknote[:, :class]); # output vector: select the class column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db49427",
   "metadata": {},
   "source": [
    "Finally, let's partition the data into a `training` and `testing` set so that we can determine how well the model can predict unseen data, i.e., how well the model generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef50d7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, testing = let\n",
    "\n",
    "    # initialize -\n",
    "    s = 0.80; # fraction of data for training\n",
    "    number_of_training_samples = Int(round(s * size(X,1))); # 80% of the data for training\n",
    "    i = randperm(size(X,1)); # random permutation of the indices\n",
    "    training_indices = i[1:number_of_training_samples]; # first 80% of the indices\n",
    "    testing_indices = i[number_of_training_samples+1:end]; # last 20% of\n",
    "    \n",
    "\n",
    "    # setup training -\n",
    "    one_vector = ones(number_of_training_samples);\n",
    "    training = (X=[X[training_indices, :] one_vector], y=y[training_indices]);\n",
    "\n",
    "    # setup testing -\n",
    "    one_vector = ones(length(testing_indices));\n",
    "    testing = (X=[X[testing_indices, :] one_vector], y=y[testing_indices]);\n",
    "\n",
    "    training, testing;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b397c3b",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca37adba",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "Let's develop a simple gradient descent algorithm for this classification problem. We'll first present the general gradient descent algorithm that can handle inequality and equality constraints, then simplify it for our specific unconstrained logistic regression problem where we only need to minimize the cross-entropy loss function $J(\\theta)$.\n",
    "\n",
    "### General gradient descent algorithm\n",
    "The general algorithm iteratively updates the parameter vector $\\theta_k$ using the gradient of an augmented objective function $P_{\\mu,\\rho}(\\theta)$ that includes penalty and barrier terms for constraints.\n",
    "\n",
    "__Initialization__: Given an initial guess $\\theta_0$, set $\\mu > 0$ and $\\rho > 0$. Specify a tolerance $\\epsilon > 0$, a maximum number of iterations $K$, and a step size (learning rate) $\\alpha > 0$. Set $\\texttt{converged} \\gets \\texttt{false}$, the iteration counter to $k \\gets 0$ and specify values for the penalty update parameters $(\\tau_{\\mu},\\tau_{\\rho})\\in\\left(0,1\\right)$.\n",
    "\n",
    "While not $\\texttt{converged}$ __do__:\n",
    "1. Compute the gradient: $\\nabla P_{\\mu,\\rho}(\\theta_k) = \\nabla f(\\theta_k) + \\frac{1}{\\mu} \\sum_{i=1}^m \\frac{\\nabla g_i(\\theta_k)}{-g_i(\\theta_k)} + \\frac{1}{\\rho} \\sum_{j=1}^p h_j(\\theta_k) \\nabla h_j(\\theta_k)$ evaluated at the current solution $\\theta_k$, where $f(\\theta)$ is the objective function, $g_i(\\theta)$ are inequality constraints, and $h_j(\\theta)$ are equality constraints.\n",
    "2. Update the solution: $\\theta_{k+1} = \\theta_k - \\alpha \\nabla P_{\\mu,\\rho}(\\theta_k)$. $\\texttt{Note}$: $\\alpha$ is fixed here, but it can be adapted dynamically based on the convergence behavior.\n",
    "3. Check convergence: \n",
    "     - If $\\|\\theta_{k+1} - \\theta_k\\|_{2} \\leq \\epsilon$, set $\\texttt{converged} \\gets \\texttt{true}$. Return $\\theta_{k+1}$ as the approximate solution. $\\texttt{Note}$: here we look at the Euclidean norm of the difference between the current and next solution. However, many other criteria can be used, such as the change in the objective function value or the gradient norm.\n",
    "     - If $k \\geq K$, set $\\texttt{converged} \\gets \\texttt{true}$. Warn that the maximum number of iterations has been reached without convergence.\n",
    "4. Increment the iteration counter: $k \\gets k + 1$, update $\\mu\\gets \\tau_\\mu\\,\\mu$ and $\\rho\\gets \\tau_\\rho\\,\\rho$ as needed, and repeat.\n",
    "\n",
    "As $\\mu\\to0$, the coefficient $\\frac{1}{\\mu}$ in the barrier term grows, creating an increasingly strong barrier that keeps the solution away from constraint boundaries (where $g_i(\\theta)\\to 0^-$). Similarly, as $\\rho\\to0$, the coefficient $\\frac{1}{\\rho}$ in the penalty term grows, enforcing $h_j(\\theta)\\to0$ ever more strictly.\n",
    "\n",
    "### Simplified algorithm for logistic regression\n",
    "For our logistic regression problem, we have no inequality constraints ($m=0$) and no equality constraints ($p=0$). Thus, the augmented objective function reduces to the original cross-entropy loss function $P_{\\mu,\\rho}(\\theta) = J(\\theta)$, and the gradient descent update simplifies to:\n",
    "$$\n",
    "\\theta_{k+1} = \\theta_k - \\alpha \\nabla J(\\theta_k)\n",
    "$$\n",
    "This is the form we'll implement below to find the optimal parameters $\\theta$ for our binary classifier.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06aeeeb",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "We implemented [the `MyLogisticRegressionClassificationModel` type](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/types/#VLDataScienceMachineLearningPackage.MyLogisticRegressionClassificationModel), which contains data required to solve the logistic regression problem, i.e., parameters, the learning rate, a stopping tolerance parameter $\\epsilon$, and a loss (objective) function that we want to minimize. \n",
    "* __Technical note__: In this implementation, we approximated the gradient calculation using [a forward finite difference](https://en.wikipedia.org/wiki/Finite_difference). In general, this is not a great idea. This is one of my super pet peeves of gradient descent; computing the gradient is usually a hassle, and we do a bunch of function evaluations to get a good approximation of the gradient. However, finite difference is easy to implement.\n",
    "* __Note on the loss function__: In the code below, we use the natural logarithm `log` in the loss function. You could also use `log10`. While this differs from the mathematical derivation above (which uses natural log), it doesn't change the location of the minimum since `log10` is simply a scaled version of the natural log. The gradient descent algorithm will find the same optimal parameters $\\theta$.\n",
    "* In the code block below, we [build a `model::MyLogisticRegressionClassificationModel` instance using a `build(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/factory/#Factory-methods). The model instance initially has a random guess for the classifier parameters. We use gradient descent to refine that guess [using the `learn(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.learn), which returns an updated model instance (with the best parameters that we found so far). \n",
    "\n",
    "We return the updated model instance and save it in the `model_logistic::MyLogisticRegressionClassificationModel` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c69f90ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped after number of iterations: 40001. We have error: 142.0130872410051\n"
     ]
    }
   ],
   "source": [
    "model_logistic = let\n",
    "\n",
    "    # data -\n",
    "    X = training.X; # feature matrix\n",
    "    y = training.y; # labels\n",
    "    number_of_features = size(X,2); # number of features + 1\n",
    "\n",
    "    # model\n",
    "    model = build(MyLogisticRegressionClassificationModel, (\n",
    "        parameters = 0.01*ones(number_of_features), # initial value for the parameters: these will be updated\n",
    "        learning_rate = 0.005, # you pick this\n",
    "        ϵ = 1e-6, # you pick this (this is also the step size for the fd approx to the gradient)\n",
    "        loss_function = (x,y,θ) -> log(1+exp(-2*y*(dot(x,θ)))) # what??!? Wow, that is nice. Yes, we can pass functions as args!\n",
    "    ));\n",
    "\n",
    "    # train -\n",
    "    model = learn(X,y,model, maxiter = 20000, verbose = true); # this is learning the model parameters\n",
    "\n",
    "    # return -\n",
    "    model;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e6f001",
   "metadata": {},
   "source": [
    "Let's use the updated `model_logistic::MyLogisticRegressionClassificationModel` instance (that has learned some parameters from the `training` data) and test how well we classify data that we have never seen, i.e., how well we classify the `test` dataset.\n",
    "\n",
    "__Inference__: We run the classification operation on the (unseen) test data [using the `classify(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.classify). This method takes a feature array `X` and the (trained) model instance. It returns the probability of a label in the `P::Array{Float64,2}` array (which is different than the Perceptron). Each row of `P` corresponds to a test instance, in which each column corresponds to a label, in the case `1` and `-1`.\n",
    "\n",
    "We store the actual (correct) label in the `y_banknote_logistic::Array{Int64,1}` vector. We compute the predicted label for each test instance by finding the highest probability column. We store the predicted labels in the `ŷ_banknote_logistic::Array{Int64,1}` vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "602a6ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ_banknote_logistic,y_banknote_logistic, P = let\n",
    "\n",
    "    # initialize -\n",
    "    X = testing.X; # feature matrix\n",
    "    y = testing.y; # labels\n",
    "    number_of_examples = size(X,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(X,2); # how many features do we have (cols) + 1\n",
    "\n",
    "    # compute the estimated labels -\n",
    "    P = classify(X, model_logistic) # logistic regression returns a x x 2 array holding the probability\n",
    "\n",
    "    # convert the probability to a choice ... for each row (test instance), compute the col with the highest probability\n",
    "    ŷ = zeros(number_of_examples);\n",
    "    for i ∈ 1:number_of_examples\n",
    "        a = argmax(P[i,:]); # col index with largest value\n",
    "        ŷ[i] = 1; # default\n",
    "        if (a == 2)\n",
    "            ŷ[i] = -1;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # return -\n",
    "    ŷ, y, P\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6f96e0a-cffd-41da-966c-6dc61aaf6bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274×2 Matrix{Float64}:\n",
       " 0.995993     0.00400706\n",
       " 3.46843e-63  1.0\n",
       " 1.0          1.65177e-26\n",
       " 1.15496e-26  1.0\n",
       " 1.0          1.13219e-40\n",
       " 3.61692e-73  1.0\n",
       " 1.0          4.60244e-36\n",
       " 1.125e-61    1.0\n",
       " 4.35644e-68  1.0\n",
       " 1.0          6.72363e-20\n",
       " 3.88299e-66  1.0\n",
       " 6.4706e-91   1.0\n",
       " 1.0          3.95856e-30\n",
       " ⋮            \n",
       " 1.0          2.27876e-19\n",
       " 1.0          9.0244e-37\n",
       " 1.7604e-42   1.0\n",
       " 1.0          6.61743e-39\n",
       " 1.82287e-61  1.0\n",
       " 2.36476e-71  1.0\n",
       " 1.0          1.2333e-34\n",
       " 1.0          5.69645e-24\n",
       " 1.0          1.17819e-35\n",
       " 1.57079e-84  1.0\n",
       " 0.042262     0.957738\n",
       " 4.31939e-76  1.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0175ef98",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "Let's now compute the __confusion matrix__. The confusion matrix for a binary classifier is typically structured as follows:\n",
    "\n",
    "|                     | **Predicted Positive** | **Predicted Negative** |\n",
    "|---------------------|------------------------|------------------------|\n",
    "| **Actual Positive** | True Positive (TP)     | False Negative (FN)    |\n",
    "| **Actual Negative** | False Positive (FP)    | True Negative (TN)     |\n",
    "\n",
    "We've implemented [the `confusion(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.confusion) to compute the confusion matrix given the actual and predicted labels. Let's save the confusion matrix in the `CM_logistic::Array{Int64,2}` variable and compute the accuracy of the classifier on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "162ab1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Int64}:\n",
       " 112    2\n",
       "   2  158"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CM_logistic = confusion(y_banknote_logistic, ŷ_banknote_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66acf415",
   "metadata": {},
   "source": [
    "Now let's compute the accuracy of the classifier on the test data. \n",
    "\n",
    "> __Overall accuracy:__ The overall accuracy is the proportion of correctly classified instances among the total instances in the test set. In our case, it is the trace of the confusion matrix (sum of the diagonal elements) divided by the total number of instances. This gives us a measure of the overall performance of the classifier, but does not tell us if we are biased towards one class or the other.\n",
    "\n",
    "What is the overall accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e03e1092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall test fraction correct: 0.9854 versus incorrect 0.0146\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    # initialize -\n",
    "    number_of_test_banknotes = length(y_banknote_logistic); # what is the total number of test banknotes\n",
    "    correct_prediction_logistic = CM_logistic[1,1] + CM_logistic[2,2]; # true positives + true negatives\n",
    "    (correct_prediction_logistic/number_of_test_banknotes) |> f-> round(f, digits=4) |> f-> println(\"Overall test fraction correct: $(f) versus incorrect $((1-f) |> x-> round(x, digits=4))\");\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debde1f4",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3804fba1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this activity, we built a logistic regression binary classifier from first principles using gradient descent optimization. \n",
    "\n",
    "We derived the cross-entropy loss function from a Boltzmann distribution, implemented the gradient descent algorithm with finite difference gradient approximations, and trained the model on the banknote authentication dataset.\n",
    "\n",
    "> __Key Takeaways:__\n",
    "> \n",
    "> * **Cross-entropy loss minimization:** We minimized the negative log-likelihood function (cross-entropy loss) to find optimal classifier parameters. The gradient descent algorithm iteratively updated parameters using a learning rate of 0.005 until convergence.\n",
    "> * **Training and testing data split:** We randomly partitioned the banknote dataset into 80% training data and 20% testing data. This split allowed us to evaluate how well the trained model generalizes to unseen examples.\n",
    "> * **Performance evaluation with confusion matrix:** We computed a confusion matrix to assess the classifier's performance on the test set. The matrix shows true positives, true negatives, false positives, and false negatives for genuine versus forged banknote classifications.\n",
    "\n",
    "The logistic regression model successfully learned to classify banknotes based on wavelet-transformed image features, demonstrating the effectiveness of gradient descent for parameter estimation in binary classification problems.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecfb4e3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.1",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
