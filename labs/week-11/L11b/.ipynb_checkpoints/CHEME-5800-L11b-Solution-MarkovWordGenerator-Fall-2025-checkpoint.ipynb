{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7782ed9",
   "metadata": {},
   "source": [
    "# L11b: Implementing a Markov Word Generator\n",
    "In this lab, we construct a program that generates random character sequences of defined length using a discrete Markov chain.\n",
    "\n",
    "> __Learning Objectives:__\n",
    ">\n",
    "> After completing this activity, students will be able to:\n",
    "> * **Build character-level transition matrices from vocabulary data:** We extract transition probabilities between consecutive characters by analyzing approximately 370K English words, creating a 26×26 transition matrix that captures the statistical patterns of character sequences in the English language.\n",
    "> * **Implement stochastic word generation using Markov chains:** We apply the transition matrix to generate random words by starting with an initial character and iteratively sampling subsequent characters according to learned transition probabilities, producing character sequences of specified length.\n",
    "> * **Validate generated sequences against empirical data:** We compare machine-generated words with the original vocabulary dataset to quantify how many generated sequences correspond to real English words, demonstrating the relationship between statistical modeling and linguistic structure.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1604255e",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "First, we set up the computational environment by including the `Include.jl` file and loading any needed resources.\n",
    "\n",
    "> The [`include(...)` command](https://docs.julialang.org/en/v1/base/base/#include) evaluates the contents of the input source file, `Include.jl`, in the notebook's global scope. The `Include.jl` file sets paths, loads required external packages, etc. For additional information on functions and types used in this material, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/). \n",
    "\n",
    "Let's set up our code environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e4e54a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(joinpath(@__DIR__, \"Include-solution.jl\")); # include the Include.jl file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b5d46e",
   "metadata": {},
   "source": [
    "In addition to standard Julia libraries, we'll also use [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl). Check out [the documentation](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/) for more information on the functions, types, and data used in this material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20eb32d",
   "metadata": {},
   "source": [
    "### Data\n",
    "We included a JSON file containing approximately 370K English words [originally downloaded from here](https://github.com/dwyl/english-words) in [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl). We'll use this data to train our Markov word generator.\n",
    "\n",
    "We've implemented a helper function to read the words from the JSON file and return the data as a Julia dictionary. To load the vocabulary data, [call the `MyEnglishLanguageVocabularyModel(...)` helper function](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/data/#VLDataScienceMachineLearningPackage.MyEnglishLanguageVocabularyModel). We'll save the vocabulary data in the `vocabulary_data_dictionary::Dict{Char, Set{String}}` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "238ffbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_data_dictionary = MyEnglishLanguageVocabularyModel();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4259dfd4",
   "metadata": {},
   "source": [
    "What's in the `vocabulary_data_dictionary` variable? Let's look at words that start with the character `a`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9026ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Set{String} with 25416 elements:\n",
       "  \"archetypes\"\n",
       "  \"acrodont\"\n",
       "  \"apogaic\"\n",
       "  \"acousma\"\n",
       "  \"acutangular\"\n",
       "  \"alteza\"\n",
       "  \"anhidrosis\"\n",
       "  \"abracadabra\"\n",
       "  \"agrostographies\"\n",
       "  \"anthropologically\"\n",
       "  \"assentation\"\n",
       "  \"attache\"\n",
       "  \"acanthad\"\n",
       "  \"aristotelism\"\n",
       "  \"alterity\"\n",
       "  \"attargul\"\n",
       "  \"arditi\"\n",
       "  \"abducted\"\n",
       "  \"amotus\"\n",
       "  \"adenomycosis\"\n",
       "  \"aleurobius\"\n",
       "  \"amphirhinal\"\n",
       "  \"anaerobic\"\n",
       "  \"amputator\"\n",
       "  \"alboranite\"\n",
       "  ⋮ "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_data_dictionary['a'] # Wow! we have > 25000 words that start with the character 'a'!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ffbf53",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b7c743",
   "metadata": {},
   "source": [
    "## Task 1: Compute the Character Transition Matrix\n",
    "In this task, we'll compute the transition matrix for our Markov chain. The transition matrix defines the probabilities of moving from one character to another in our generated words.\n",
    "\n",
    "> __Helper:__ We've implemented [the `vocabulary_transition_matrix(...)` helper function](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/text/#VLDataScienceMachineLearningPackage.vocabulary_transition_matrix) to compute the transition matrix from the vocabulary data. The function takes the vocabulary data and a list of characters to consider (e.g., the lowercase letters `a` to `z`) and returns the transition matrix as a `Matrix{Float64}`. \n",
    "\n",
    "Let's compute the transition matrix and save it in the `P::Array{Float64, 2}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "941f6dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "P, characters = let\n",
    "\n",
    "    # initialize - \n",
    "    P = nothing;\n",
    "    characters = Array{Char,1}();\n",
    "\n",
    "    # build the character list and transition matrix -\n",
    "    characters = [Char(i) for i in 'a':'z']; # list of characters to consider\n",
    "    P = vocabulary_transition_matrix(vocabulary_data_dictionary, characters)\n",
    "\n",
    "    P, characters # return the transition matrix, and the character list\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81da7c1",
   "metadata": {},
   "source": [
    "The transition matrix $\\mathbf{P}$ is a `26 x 26` matrix, where the rows and columns correspond to the characters `a` to `z`. Each entry $P_{ij}$ represents the probability of transitioning from character `i` to character `j` computed from the vocabulary data.\n",
    "\n",
    "Let's examine a few transition probabilities from the matrix $\\mathbf{P}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f144d194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition probability P('b' -> 'r') = 0.1395\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    # initialize -\n",
    "    start_char = 'b'; # starting character (change this to see other characters)\n",
    "    next_char = 'r'; # next character (change this to see other characters)\n",
    "    i = findfirst(x -> x == start_char, characters); # index of the start_char\n",
    "    j = findfirst(x -> x == next_char, characters); # index of the next_char\n",
    "\n",
    "    # print the transition probability -\n",
    "    println(\"Transition probability P('$start_char' -> '$next_char') = $(P[i, j] |> x-> round(x, digits=4))\");\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d39e50f",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "What's the highest transition probability from character i to character j? You can find out by inspecting the matrix $\\mathbf{P}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ddd2239",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    # TODO: implement the discussion question code in this cell.\n",
    "    # throw(ErrorException(\"Ooops! Implement the analysis of the P matrix here!\"))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb7acd6",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5279538",
   "metadata": {},
   "source": [
    "## Task 2: Generate Random Words One Character at a Time\n",
    "In this task, we'll generate random words using the transition matrix we computed in Task 1. We'll start with an initial character and then use the transition probabilities to select subsequent characters until we reach the desired word length.\n",
    "\n",
    "> __Helper:__ We've implemented [the `sample_words(...)` helper function](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/text/#VLDataScienceMachineLearningPackage.sample_words) to generate random character sequences by iteratively sampling from the transition matrix. The function takes the transition matrix `P::Array{Float64,2}`, a list of characters `characters::Vector{Char}`, and generates `number_of_samples::Int` words of length `length_of_sample_word::Int` starting from `startchar::Char`.\n",
    "\n",
    "Let's generate 1,000 random six-character words starting with the letter `b` and store them in `generated_words_set::Set{String}`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d5d3965",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_words_set = let\n",
    "\n",
    "    # initialize -\n",
    "    number_of_samples = 1000;\n",
    "    length_of_sample_word = 6;\n",
    "    sample_word_start_char = 'b';\n",
    "\n",
    "    # generate samples -\n",
    "    S = sample_words(P, characters, number_of_samples = number_of_samples, \n",
    "        length_of_sample_word = length_of_sample_word, startchar = sample_word_start_char);\n",
    "\n",
    "    # store the generated words in a Set to avoid duplicates -\n",
    "    generated_words_set = Set{String}();\n",
    "    for (i, word) ∈ S\n",
    "        push!(generated_words_set, word);\n",
    "    end\n",
    "\n",
    "    generated_words_set; # return the generated words\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347059b6",
   "metadata": {},
   "source": [
    "Wow! That's kind of fun. But, I'm curious: how many of the generated words are actually real English words? Let's check the generated words against our vocabulary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63795cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of samples present in the vocabulary model: 0.9626955475330927%\n"
     ]
    }
   ],
   "source": [
    "true_generated_words = let\n",
    "\n",
    "    # initialize -\n",
    "    S = generated_words_set; # generated words\n",
    "    sample_word_start_char = 'b';\n",
    "    my_word_set = vocabulary_data_dictionary[sample_word_start_char];\n",
    "    number_of_samples = length(generated_words_set);\n",
    "    true_generated_words = Set{String}();\n",
    "\n",
    "    for word ∈ S\n",
    "        if word ∈ my_word_set\n",
    "            push!(true_generated_words, word);\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    N₊ = length(true_generated_words); # number of true words, that are in the vocabulary model\n",
    "    println(\"Fraction of samples present in the vocabulary model: $((N₊/number_of_samples)*100)%\");\n",
    "    true_generated_words; # return the true words\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4ce67d",
   "metadata": {},
   "source": [
    "__Interesting:__ Depending on the initial character and the random choices made during generation, the number of real words may vary. However, you should see that a small but significant fraction of the generated words are indeed real English words. This fraction depends on many factors, including the length of the requested words and the initial character.\n",
    "\n",
    "Let's look at some of the __actual__ words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6708014",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: set must be non-empty",
     "output_type": "error",
     "traceback": [
      "ArgumentError: set must be non-empty",
      "",
      "Stacktrace:",
      " [1] \u001b[0m\u001b[1mpop!\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90ms\u001b[39m::\u001b[0mSet\u001b[90m{String}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m",
      "\u001b[90m   @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mset.jl:160\u001b[24m\u001b[39m",
      " [2] top-level scope",
      "\u001b[90m   @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[22]:9\u001b[24m\u001b[39m",
      " [3] \u001b[0m\u001b[1meval\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mm\u001b[39m::\u001b[0mModule, \u001b[90me\u001b[39m::\u001b[0mAny\u001b[0m\u001b[1m)\u001b[22m",
      "\u001b[90m   @\u001b[39m \u001b[90mCore\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mboot.jl:489\u001b[24m\u001b[39m"
     ]
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    # initialize -\n",
    "    df = DataFrame();\n",
    "    number_of_words_to_look_at = 10; # how many do we want to see?\n",
    "   \n",
    "    # populate the data frame -\n",
    "    for i ∈ 1:number_of_words_to_look_at\n",
    "        row_df = (\n",
    "            index = i,\n",
    "            true_word = pop!(true_generated_words)\n",
    "        );\n",
    "        push!(df, row_df);\n",
    "    end    \n",
    "\n",
    "    # make a table -\n",
    "    pretty_table(\n",
    "         df;\n",
    "         backend = :text,\n",
    "         table_format = TextTableFormat(borders = text_table_borders__compact)\n",
    "    );\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4750fa",
   "metadata": {},
   "source": [
    "There are many interesting questions to consider:\n",
    "\n",
    "> __Future Directions:__\n",
    "> 1. We get a number of \"real\" words, but we have a potential problem: when we check for real words, we could have false negatives. For example, suppose we generate an actual word that is not in our vocabulary data. How do we estimate the probability of getting a false negative when we check for real words?\n",
    "> 2. How could we improve our Markov word generator? For example, what if we considered pairs of characters (e.g., \"th\", \"he\", \"in\", etc.) instead of single characters? How would that change our transition matrix and the generated words?\n",
    "\n",
    "This is only a starting point. There are many ways to extend and improve this Markov word generator. Feel free to explore and experiment with different approaches!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be21b01",
   "metadata": {},
   "source": [
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9d8d20",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this activity, we constructed a character-level Markov word generator by computing transition probabilities from a large English vocabulary dataset, generated random word sequences, and validated the linguistic plausibility of the output.\n",
    "\n",
    "> __Key Takeaways:__\n",
    "> \n",
    "> * **Statistical learning from corpus data:** We built a 26×26 character transition matrix by analyzing patterns in 370K English words, demonstrating how Markov models can capture the statistical structure of language through observed character co-occurrence frequencies in real linguistic data.\n",
    "> * **Stochastic sequence generation algorithm:** We implemented a forward sampling approach that starts with an initial character and iteratively selects subsequent characters by sampling from categorical distributions defined by the transition matrix, producing random yet statistically plausible character sequences.\n",
    "> * **Empirical validation of generative models:** We verified that a measurable fraction of randomly generated sequences matched actual English words in the vocabulary, illustrating how simple probabilistic models can approximate complex linguistic patterns while also revealing limitations through the presence of non-word sequences.\n",
    "\n",
    "This Markov word generator demonstrates fundamental principles of statistical language modeling, with natural extensions to higher-order models (bigrams, trigrams) and applications in text generation, spell correction, and computational linguistics.\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.1",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
