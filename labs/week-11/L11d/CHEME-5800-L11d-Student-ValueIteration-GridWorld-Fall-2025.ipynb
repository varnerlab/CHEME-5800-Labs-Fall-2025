{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74c7da3d-5bac-4d7d-81c2-f135ae11d8cc",
   "metadata": {},
   "source": [
    "## L11d: Value Iteration for Grid World Navigation\n",
    "In this lab, you will implement the value iteration algorithm to solve a two-dimensional grid world navigation task where a robot must find the optimal path from any starting position to a charging station while avoiding lava pits. However, like everything in life, there is a backstory.\n",
    "\n",
    "__Backstory:__ We have a value iteration algorithm that we took from [the Algorithms for Decision Making book](https://algorithmsbook.com/decisionmaking/), which is written by some folks at Stanford University. However, the code is convoluted and hard to follow (we've included it here). Your task is to refactor the value iteration code into a simpler version that is easier to understand and closer to the pseudocode that we discussed in lecture (with a convergence check, clear Bellman update step, etc.). Once you have your refactored version working, you will compare the results to ensure that your implementation produces the same optimal value function as the original.\n",
    "\n",
    "\n",
    "\n",
    "> __Learning Objectives:__\n",
    ">\n",
    "> After completing this activity, students will be able to:\n",
    "> * **Construct Markov Decision Process (MDP) models for spatial navigation:** We build a 20√ó20 grid world MDP with states representing grid positions, actions for movement (left, right, up, down), rewards for reaching goals or hazards, and transition dynamics encoding deterministic movement.\n",
    "> * **Implement value iteration with Bellman updates:** We iteratively compute the optimal value function $U^*(s)$ using the Bellman backup operation, which propagates utility values backward from high-reward states until convergence, enabling optimal decision-making from any state.\n",
    "> * **Extract optimal policies from value functions:** We derive the optimal policy $\\pi^*(s)$ by computing the action-value function $Q(s,a)$ from the converged value function and selecting the action that maximizes expected future reward at each state, then visualize the resulting navigation paths.\n",
    "\n",
    "This example demonstrates how value iteration solves sequential decision problems by computing optimal policies that balance immediate rewards with long-term consequences. Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b91c6d-e13e-4b8c-bf14-d4651ef03d5e",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "First, we set up the computational environment by including the `Include.jl` file and loading any needed resources.\n",
    "\n",
    "> The [`include(...)` command](https://docs.julialang.org/en/v1/base/base/#include) evaluates the contents of the input source file, `Include.jl`, in the notebook's global scope. The `Include.jl` file sets paths, loads required external packages, etc. For additional information on functions and types used in this material, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/). \n",
    "\n",
    "Let's set up our code environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de6de82-32d0-4ea2-93b5-6c55c94d2e8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "include(joinpath(@__DIR__, \"Include-student.jl\")); # include the Include.jl file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa90108b-9ca9-4ab9-aba1-5085799c43fd",
   "metadata": {},
   "source": [
    "In addition to standard Julia libraries, we'll also use [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl). Check out [the documentation](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/) for more information on the functions, types, and data used in this material.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af3c291",
   "metadata": {},
   "source": [
    "## Task 1: Build the Grid World Model\n",
    "We encode the rectangular grid world using [the `MyRectangularGridWorldModel` type](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/mdp/#VLDataScienceMachineLearningPackage.MyRectangularGridWorldModel), which represents a spatial environment where a robot must navigate from any starting position to a charging station while avoiding lava pits. \n",
    "\n",
    "> __Grid World Structure:__ The model consists of:\n",
    "> * A 20√ó20 grid with $(x,y)$ coordinates mapping to discrete states\n",
    "> * Positive rewards for reaching the charging station (goal)\n",
    "> * Large negative penalties for lava pits (hazards)\n",
    "> * Small step costs for regular movements (efficiency incentive)\n",
    "> * Terminal absorbing states where episodes end (charging station and lava pits)\n",
    "\n",
    "Let's set up the world parameters by defining the grid dimensions, the number of available actions (left, right, up, down), and the discount factor $\\gamma$ that controls how much we value future rewards relative to immediate rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f4d2229-9484-4320-ad55-3c597a8b3881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_rows = 20; # number of rows in the grid world (you can change this)\n",
    "number_of_cols = 20; # number of cols in the grid world (you can change this)\n",
    "nactions = 4; # number of actions (LRUD)\n",
    "Œ≥ = 0.95; # discount factor (you can change this)\n",
    "nstates = (number_of_rows*number_of_cols);\n",
    "ùíÆ = range(1,stop=nstates,step=1) |> collect; # states\n",
    "ùíú = range(1,stop=nactions,step=1) |> collect; # actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8205431c",
   "metadata": {},
   "source": [
    "What's in the states and actions arrays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1714e902-b72d-479d-bac6-537c7fa33649",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400-element Vector{Int64}:\n",
       "   1\n",
       "   2\n",
       "   3\n",
       "   4\n",
       "   5\n",
       "   6\n",
       "   7\n",
       "   8\n",
       "   9\n",
       "  10\n",
       "   ‚ãÆ\n",
       " 392\n",
       " 393\n",
       " 394\n",
       " 395\n",
       " 396\n",
       " 397\n",
       " 398\n",
       " 399\n",
       " 400"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ùíÆ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea7b15-294b-485a-8b59-3442fc339f06",
   "metadata": {},
   "source": [
    "Next, we define the reward structure by creating the `rewards::Dict{Tuple{Int,Int}, Float64}` dictionary, which maps $(x,y)$ coordinates to reward values. We set the lava pit penalty to $-1000$ for dangerous locations that destroy the robot, while the charging station reward is $+100$ for successfully reaching the goal. We also define an `absorbing_state_set::Set{Tuple{Int,Int}}` containing locations where the episode terminates. Note that we only specify non-default rewards in this dictionary since default step costs will be added later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "585ea9b1-a9f9-4628-9bb7-f43d7811f970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup rewards -\n",
    "lava_reward = -1000.0;\n",
    "charging_reward = 100.0\n",
    "\n",
    "rewards = Dict{Tuple{Int,Int}, Float64}()\n",
    "rewards[(1,2)] = lava_reward # lava in the (1,2) square \n",
    "rewards[(2,2)] = lava_reward # lava in the (2,2) square \n",
    "rewards[(2,3)] = lava_reward # lava in the (2,3) square \n",
    "rewards[(4,5)] = lava_reward # lava in the (4,5) square\n",
    "rewards[(19,20)] = lava_reward # lava in the (19,20) square\n",
    "rewards[(4,3)] = charging_reward   # charging station square\n",
    "\n",
    "# setup set of absorbing states -\n",
    "absorbing_state_set = Set{Tuple{Int,Int}}()\n",
    "for (k,v) ‚àà rewards\n",
    "    push!(absorbing_state_set, k);   \n",
    "end\n",
    "\n",
    "# setup soft walls (constraints) -\n",
    "soft_wall_set = Set{Tuple{Int,Int}}();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcf87c2-0b91-458e-8fb0-bb0db4e451ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we build an instance of [the `MyRectangularGridWorldModel` type](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/mdp/#VLDataScienceMachineLearningPackage.MyRectangularGridWorldModel), which encapsulates the complete grid world environment. We pass the grid dimensions and reward structure to the `build(...)` method, which constructs the `world::MyRectangularGridWorldModel` instance containing the coordinate system (mapping between $(x,y)$ positions and discrete state indices), valid state transitions (moves from each position), and the complete reward function (including default step costs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80f48d7a-a171-418d-8e99-cedb894ef305",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "world = build(MyRectangularGridWorldModel, \n",
    "    (nrows = number_of_rows, ncols = number_of_cols, rewards = rewards));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7af1d83-8feb-47ff-b100-1bc01bfe7ef9",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's inspect the world model structure. The `world.moves::Dict{Int,Tuple{Int,Int}}` dictionary maps action indices to movement vectors $\\Delta = (\\Delta_x, \\Delta_y)$, which describe how each action changes the robot's position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d4fbd99-9eb9-4c8c-a245-e6bc7151b8e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Int64, Tuple{Int64, Int64}} with 4 entries:\n",
       "  4 => (0, 1)\n",
       "  2 => (1, 0)\n",
       "  3 => (0, -1)\n",
       "  1 => (-1, 0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "world.moves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ed737-187b-4c88-88e4-dcca9d79e66a",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b5c9c",
   "metadata": {},
   "source": [
    "## Task 2: Construct the MDP Components\n",
    "To apply value iteration, we need to construct the formal MDP components from our grid world model. \n",
    "\n",
    "> __MDP Definition:__ A Markov Decision Process is defined by the tuple $\\left(\\mathcal{S}, \\mathcal{A}, R\\left(s, a\\right), T\\left(s^{\\prime}\\,|\\,s,a\\right), \\gamma\\right)$ where:\n",
    "> * $\\mathcal{S}$ is the state space (all 400 grid positions in our 20√ó20 grid)\n",
    "> * $\\mathcal{A}$ is the action space (left, right, up, down movements)\n",
    "> * $R\\left(s,a\\right)$ is the reward function (immediate rewards for state-action pairs)\n",
    "> * $T\\left(s^{\\prime}\\,|\\,s,a\\right)$ is the transition model (probability of reaching $s^{\\prime}$ from $s$ via action $a$)\n",
    "> * $\\gamma = 0.95$ is the discount factor (weighting future rewards relative to immediate rewards)\n",
    "\n",
    "Let's construct these components systematically, starting with the reward function.\n",
    "\n",
    "### Reward Function $R\\left(s,a\\right)$\n",
    "We encode the reward function as an `R::Array{Float64,2}` matrix with dimensions $|\\mathcal{S}| \\times |\\mathcal{A}|$ where entry $R[s,a]$ gives the immediate reward for taking action $a$ from state $s$. \n",
    "\n",
    "> __Reward Structure:__ The rewards are assigned as follows:\n",
    "> * Charging station: $+100$ (goal state)\n",
    "> * Lava pits: $-1000$ (dangerous states)\n",
    "> * Regular steps: $-1$ (small cost to encourage efficiency)\n",
    "> * Out-of-bounds: $-50000$ (large penalty for invalid moves)\n",
    "\n",
    "For each state-action pair, we compute the resulting position and assign the appropriate reward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f51634b4-76cf-4533-81d2-60e1470117dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400√ó4 Matrix{Float64}:\n",
       " -50000.0      -1.0  -50000.0   -1000.0\n",
       " -50000.0   -1000.0      -1.0      -1.0\n",
       " -50000.0   -1000.0   -1000.0      -1.0\n",
       " -50000.0      -1.0      -1.0      -1.0\n",
       " -50000.0      -1.0      -1.0      -1.0\n",
       " -50000.0      -1.0      -1.0      -1.0\n",
       " -50000.0      -1.0      -1.0      -1.0\n",
       " -50000.0      -1.0      -1.0      -1.0\n",
       " -50000.0      -1.0      -1.0      -1.0\n",
       " -50000.0      -1.0      -1.0      -1.0\n",
       "      ‚ãÆ                        \n",
       "     -1.0  -50000.0      -1.0      -1.0\n",
       "     -1.0  -50000.0      -1.0      -1.0\n",
       "     -1.0  -50000.0      -1.0      -1.0\n",
       "     -1.0  -50000.0      -1.0      -1.0\n",
       "     -1.0  -50000.0      -1.0      -1.0\n",
       "     -1.0  -50000.0      -1.0      -1.0\n",
       "     -1.0  -50000.0      -1.0      -1.0\n",
       "     -1.0  -50000.0      -1.0      -1.0\n",
       "  -1000.0  -50000.0      -1.0  -50000.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "R = let\n",
    "\n",
    "    # initialize -\n",
    "    R = zeros(nstates, nactions);\n",
    "    fill!(R, 0.0)\n",
    "    \n",
    "    # For each state-action pair, compute the resulting position and assign rewards\n",
    "    for s ‚àà ùíÆ\n",
    "        for a ‚àà ùíú\n",
    "        \n",
    "            # Get the movement vector for this action (e.g., up = (0,1), down = (0,-1))\n",
    "            Œî = world.moves[a];\n",
    "            current_position = world.coordinates[s] # Current (x,y) position\n",
    "            new_position =  current_position .+ Œî   # Resulting (x,y) after taking action a\n",
    "            \n",
    "            # Check if the new position is valid (within grid bounds)\n",
    "            if (haskey(world.states, new_position) == true)\n",
    "                # Check if there's a special reward at this position (lava or charging station)\n",
    "                if (haskey(rewards, new_position) == true)\n",
    "                    R[s,a] = rewards[new_position]; # Assign special reward (+100 or -1000)\n",
    "                else\n",
    "                    R[s,a] = -1.0; # Regular step cost (encourages efficiency)\n",
    "                end\n",
    "            else\n",
    "                R[s,a] = -50000.0; # Large penalty for trying to move off the grid\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    R; # return R\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40727829-a6d9-446a-8384-83828d967028",
   "metadata": {},
   "source": [
    "### Transition Function $T\\left(s^{\\prime}\\,|\\,s,a\\right)$\n",
    "Next, we construct the transition function `T::Array{Float64,3}`, which describes the dynamics of the environment. We encode this as a $|\\mathcal{S}| \\times |\\mathcal{S}| \\times |\\mathcal{A}|$ array where $T[s, s^{\\prime}, a]$ gives the probability of transitioning to state $s^{\\prime}$ when taking action $a$ from state $s$. \n",
    "\n",
    "> __Deterministic Transitions:__ For our grid world:\n",
    "> * If action $a$ from state $s$ leads to a valid new position (inside the grid and not from an absorbing state), then $T[s, s^{\\prime}, a] = 1$ where $s^{\\prime}$ is the resulting state\n",
    "> * If the action would take us outside the grid or we're already in an absorbing state, then $T[s, s, a] = 1$ (we remain in the current state)\n",
    "\n",
    "This captures the physics of the world where actions have predictable outcomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13d92d6f-fcd5-44be-acc3-58a275d2987e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400√ó400√ó4 Array{Float64, 3}:\n",
       "[:, :, 1] =\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ‚Ä¶  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ‚Ä¶  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ‚ãÆ                        ‚ãÆ              ‚ã±            ‚ãÆ                   \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ‚Ä¶  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[:, :, 2] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ‚Ä¶  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ‚Ä¶  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ‚ãÆ                        ‚ãÆ              ‚ã±            ‚ãÆ                   \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ‚Ä¶  0.0  0.0  1.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  1.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
       "\n",
       "[:, :, 3] =\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ‚Ä¶  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ‚Ä¶  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ‚ãÆ                        ‚ãÆ              ‚ã±            ‚ãÆ                   \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ‚Ä¶  0.0  1.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  1.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  1.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  0.0\n",
       "\n",
       "[:, :, 4] =\n",
       " 0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ‚Ä¶  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ‚Ä¶  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ‚ãÆ                        ‚ãÆ              ‚ã±            ‚ãÆ                   \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  1.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ‚Ä¶  0.0  0.0  0.0  1.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "T = let\n",
    "\n",
    "    # initialize - 3D array with dimensions |S| x |S| x |A|\n",
    "    T = Array{Float64,3}(undef, nstates, nstates, nactions);\n",
    "    fill!(T, 0.0)\n",
    "    \n",
    "    # Build transition probabilities for each action\n",
    "    for a ‚àà ùíú\n",
    "    \n",
    "        Œî = world.moves[a]; # Get movement vector for action a\n",
    "    \n",
    "        for s ‚àà ùíÆ\n",
    "            current_position = world.coordinates[s] # Current (x,y) position\n",
    "            new_position =  current_position .+ Œî   # Resulting (x,y) after action a\n",
    "            \n",
    "            # Check if move is valid (within bounds) and not from an absorbing state\n",
    "            if (haskey(world.states, new_position) == true && \n",
    "                    in(current_position, absorbing_state_set) == false)\n",
    "                # Valid move: transition to new state with probability 1.0\n",
    "                s‚Ä≤ = world.states[new_position];\n",
    "                T[s, s‚Ä≤,  a] = 1.0\n",
    "            else\n",
    "                # Invalid move or absorbing state: stay in current state with probability 1.0\n",
    "                T[s, s,  a] = 1.0\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    T; # return T\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c291884-62b4-4b4b-8646-ff5fb0b7447a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum(T[24,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3b90c3",
   "metadata": {},
   "source": [
    "### Sanity Check: Validate Transition Probabilities\n",
    "Let's verify that our transition function is properly constructed. For a valid probability distribution, the transition probabilities from each state-action pair must sum to 1.0 across all possible next states. This ensures we have a well-formed MDP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7dd08eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All transition probabilities are valid (sum to 1.0)\n",
      "‚úì Transition function T is properly normalized\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "    # Check that transition probabilities sum to 1.0 for all state-action pairs\n",
    "    all_valid = true\n",
    "    for s ‚àà ùíÆ\n",
    "        for a ‚àà ùíú\n",
    "            prob_sum = sum(T[s, :, a])\n",
    "            if !isapprox(prob_sum, 1.0, atol=1e-10)\n",
    "                println(\"Warning: T[$s, :, $a] sums to $prob_sum (should be 1.0)\")\n",
    "                all_valid = false\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    if all_valid\n",
    "        println(\"‚úì All transition probabilities are valid (sum to 1.0)\")\n",
    "        println(\"‚úì Transition function T is properly normalized\")\n",
    "    else\n",
    "        println(\"‚úó Some transition probabilities do not sum to 1.0\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f983f3d-753a-4e5a-92f5-52f941a4d53a",
   "metadata": {},
   "source": [
    "Finally, we package all MDP components into an instance of [the `MyMDPProblemModel` type](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/mdp/#VLDataScienceMachineLearningPackage.MyMDPProblemModel). This `m::MyMDPProblemModel` instance encapsulates everything needed to solve the decision problem, including the state space $\\mathcal{S}$, action space $\\mathcal{A}$, transition dynamics $T$, reward function $R$, and discount factor $\\gamma$. The [`build(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/factory/) constructs the complete MDP representation, which we'll use with the value iteration algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef79abec-c27e-49ac-acf4-be56211c3f58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = build(MyMDPProblemModel, (\n",
    "    ùíÆ = ùíÆ, # states\n",
    "    ùíú = ùíú, # actions\n",
    "    T = T, # transition model\n",
    "    R = R, # reward model\n",
    "    Œ≥ = Œ≥ # discount factor\n",
    "));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff93bf0",
   "metadata": {},
   "source": [
    "Check out the fields of the `m::MyMDPProblemModel` instance to see the MDP components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0d40706-61bf-4927-8b0a-8fa54a829c36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400-element Vector{Int64}:\n",
       "   1\n",
       "   2\n",
       "   3\n",
       "   4\n",
       "   5\n",
       "   6\n",
       "   7\n",
       "   8\n",
       "   9\n",
       "  10\n",
       "   ‚ãÆ\n",
       " 392\n",
       " 393\n",
       " 394\n",
       " 395\n",
       " 396\n",
       " 397\n",
       " 398\n",
       " 399\n",
       " 400"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m.ùíÆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4044aa-b380-4a8c-9252-eae71914e1e3",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc24487",
   "metadata": {},
   "source": [
    "## Task 3: Compute the Optimal Value Function Using Value Iteration\n",
    "Value iteration is a dynamic programming algorithm that computes the optimal value function $U^{*}(s)$ by iteratively applying the Bellman backup operation:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "U_{k+1}(s) = \\max_{a\\in\\mathcal{A}}\\left(\\underbrace{R(s,a)}_{\\text{= now}} + \n",
    "\\gamma\\;\\overbrace{\\sum_{s^{\\prime}\\in\\mathcal{S}}T\\left(s^{\\prime}\\,|\\,s,a\\right)\\cdot{U}_{k}(s^{\\prime})}^{\\text{= future}}\\right)\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "As $k \\to \\infty$, the value function is __guaranteed to converge__ for $0\\leq\\gamma<1$ such that $U_k(s) \\to U^{*}(s)$. The optimal value function represents the maximum expected cumulative discounted reward achievable from each state under the best possible policy. Let's develop the value iteration algorithm for computing the optimal value function $U^{*}(s)$ and policy $\\pi^{*}(s)$.\n",
    "\n",
    "__Initialize__: Given an MDP with state space $\\mathcal{S}$, action space $\\mathcal{A}$, reward function $R(s,a)$, transition model $T\\left(s^{\\prime}\\,|\\,s,a\\right)$, discount factor $\\gamma$, tolerance parameter $\\epsilon$, and maximum number of iterations $T$. Initialize the iteration counter $k\\gets 0$, the initial value function $U_{0}(s) \\gets 0$ for all $s \\in \\mathcal{S}$, and $\\texttt{converged}\\gets\\texttt{false}$.\n",
    "\n",
    "While $\\texttt{converged}$ is $\\texttt{false}$ __do__:\n",
    "1. For each state $s \\in \\mathcal{S}$, compute the updated value:\n",
    "   $$U_{k+1}(s) \\gets \\max_{a\\in\\mathcal{A}}\\left(R(s,a) + \\gamma\\sum_{s^{\\prime}\\in\\mathcal{S}}T\\left(s^{\\prime}\\,|\\,s,a\\right)\\cdot{U}_{k}(s^{\\prime})\\right)$$\n",
    "2. Check for convergence:\n",
    "    - If $\\max_{s\\in\\mathcal{S}} \\left|U_{k+1}(s) - U_{k}(s)\\right| \\leq \\epsilon$, then set $\\texttt{converged}\\gets\\texttt{true}$ and $U^{*}\\gets{U}_{k+1}$.\n",
    "    - If $\\max_{s\\in\\mathcal{S}} \\left|U_{k+1}(s) - U_{k}(s)\\right| > \\epsilon$, update $k\\gets{k+1}$ and $U_{k}\\gets{U}_{k+1}$.\n",
    "3. Update the $\\texttt{converged}$ flag:\n",
    "    - If $k\\geq{T}$, then set $\\texttt{converged}\\gets\\texttt{true}$ and $U^{*}\\gets{U}_{k+1}$. Notify the caller that the maximum iteration limit was reached without convergence.\n",
    "\n",
    "__Extract Policy__: For each state $s \\in \\mathcal{S}$, compute:\n",
    "$$\\pi^{*}(s) \\gets \\arg\\max_{a\\in\\mathcal{A}}\\left(R(s,a) + \\gamma\\sum_{s^{\\prime}\\in\\mathcal{S}}T\\left(s^{\\prime}\\,|\\,s,a\\right)\\cdot{U^{*}}(s^{\\prime})\\right)$$\n",
    "\n",
    "Now let's configure and run the value iteration algorithm by creating a `value_iteration_model::MyValueIterationModel` instance with a maximum iteration limit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c50a8314-d55e-4f0c-981b-f7a406843ca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "value_iteration_model = MyValueIterationModel(1000); # maximum iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1629357-940e-4570-a267-8f978426e82a",
   "metadata": {},
   "source": [
    "Now we execute the value iteration algorithm [by calling the `solve(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/mdp/#VLDataScienceMachineLearningPackage.solve-Tuple{MyValueIterationModel,%20MyMDPProblemModel}) with our `value_iteration_model::MyValueIterationModel` instance and MDP problem specification `m::MyMDPProblemModel`. \n",
    "\n",
    "> __Stanford versus our version:__ The [`solve(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/mdp/#VLDataScienceMachineLearningPackage.solve-Tuple{MyValueIterationModel,%20MyMDPProblemModel}) has a different implementation of the value iteration algorithm that does not include convergence checking as described. The [Stanford version of the `solve(...)` method](https://algorithmsbook.com/decisionmaking/) is convoluted and hard to follow, so you need to implement our own simplified version called `mysolve(...)` that is easier to understand and closer to the pseudocode outlined above, including the Bellman update step and convergence checking criteria.\n",
    "\n",
    "Both versions return a `solution::MyValueIterationSolution` instance containing the optimal value function in the `U::Array{Float64,1}` field. Does our refactored value iteration implementation produce the same optimal value function as the original? Let's compare the results of calling both methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b9b5bc5-d9a3-48c1-8eac-13cb595feea4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "Oooops! The `mysolve(...)` function is not yet implemented. Please implement it as per the lab instructions.",
     "output_type": "error",
     "traceback": [
      "Oooops! The `mysolve(...)` function is not yet implemented. Please implement it as per the lab instructions.\n",
      "\n",
      "Stacktrace:\n",
      " [1] mysolve(model::MyValueIterationModel, problem::MyMDPProblemModel; œµ::Float64)\n",
      "   @ Main ~/Desktop/julia_work/CHEME-5800-Instances/Fall-2025/CHEME-5800-Labs-Fall-2025/labs/week-11/L11d/src/Compute.jl:104\n",
      " [2] mysolve(model::MyValueIterationModel, problem::MyMDPProblemModel)\n",
      "   @ Main ~/Desktop/julia_work/CHEME-5800-Instances/Fall-2025/CHEME-5800-Labs-Fall-2025/labs/week-11/L11d/src/Compute.jl:93\n",
      " [3] top-level scope\n",
      "   @ ~/Desktop/julia_work/CHEME-5800-Instances/Fall-2025/CHEME-5800-Labs-Fall-2025/labs/week-11/L11d/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X42sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "solution = mysolve(value_iteration_model, m); # The solve(...) method is the original Stanford version, the mysolve(...) method is our refactored version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea3eca1a-d9b8-4a10-b7bf-fde455b51347",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `solution` not defined in `Main`\nSuggestion: add an appropriate import or assignment. This global was declared but not assigned.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `solution` not defined in `Main`\n",
      "Suggestion: add an appropriate import or assignment. This global was declared but not assigned.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Desktop/julia_work/CHEME-5800-Instances/Fall-2025/CHEME-5800-Labs-Fall-2025/labs/week-11/L11d/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X43sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "solution.U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f318f4-c389-49b5-8d46-08e1df83e58f",
   "metadata": {},
   "source": [
    "### Extract the Action-Value Function $Q(s,a)$\n",
    "From the optimal value function $U^{*}(s)$, we compute the action-value function $Q(s,a)$ using the formula:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "Q^{*}(s,a) = R(s,a) + \\gamma\\sum_{s^{\\prime}\\in\\mathcal{S}}T\\left(s^{\\prime}\\,|\\,s,a\\right)\\cdot{U^{*}}(s^{\\prime})\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "This represents the expected return of taking action $a$ from state $s$ and then following the optimal policy. The [`QM(...)` function](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/mdp/#VLDataScienceMachineLearningPackage.Q-Tuple{MyMDPProblemModel,%20Vector{Float64}}) computes this for all state-action pairs, creating a `my_Q::Array{Float64,2}` matrix with dimensions $|\\mathcal{S}| \\times |\\mathcal{A}|$. This function evaluates each action's quality by considering both the immediate reward and the discounted value of resulting states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b7e1e12-f637-4ecb-af32-cd0b3fd56fdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `solution` not defined in `Main`\nSuggestion: add an appropriate import or assignment. This global was declared but not assigned.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `solution` not defined in `Main`\n",
      "Suggestion: add an appropriate import or assignment. This global was declared but not assigned.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Desktop/julia_work/CHEME-5800-Instances/Fall-2025/CHEME-5800-Labs-Fall-2025/labs/week-11/L11d/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X45sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "my_Q = QM(m, solution.U)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fc0626-8041-40e9-b92d-491b6edfe9dd",
   "metadata": {},
   "source": [
    "### Compute the Optimal Policy $\\pi^{*}(s)$\n",
    "The optimal policy is defined as: $\\pi^{*}(s) = \\arg\\max_{a\\in\\mathcal{A}}\\,Q^{*}(s,a)$. This selects the action that maximizes the action-value function at each state. \n",
    "\n",
    "The [`mypolicy(...)` function](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/mdp/#VLDataScienceMachineLearningPackage.policy-Tuple{Matrix{Float64}}) extracts this optimal policy by finding the action with the highest $Q$-value at each state, returning a `my_œÄ::Array{Int64,1}` vector that gives us the best action to take from every position in the grid. \n",
    "\n",
    "What do we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e41ba794-1995-48d1-95d5-5aceefbafbab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `my_Q` not defined in `Main`\nSuggestion: add an appropriate import or assignment. This global was declared but not assigned.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `my_Q` not defined in `Main`\n",
      "Suggestion: add an appropriate import or assignment. This global was declared but not assigned.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Desktop/julia_work/CHEME-5800-Instances/Fall-2025/CHEME-5800-Labs-Fall-2025/labs/week-11/L11d/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X50sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "my_œÄ = mypolicy(my_Q) # we rename the local version mypolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1fcde3-b949-4469-ad96-d51f978a28a8",
   "metadata": {},
   "source": [
    "### Visualize the Optimal Navigation Policy\n",
    "Now that we have the optimal policy $\\pi^{*}(s)$, let's visualize the optimal policy by simulating a path from a starting position to an absorbing state. The visualization shows the charging station (goal) as green circles, lava pits (hazards) as red circles, and the trajectory following the optimal policy $\\pi^{*}(s)$ as a blue path with arrows. \n",
    "\n",
    "Specify a `startstate::Tuple{Int,Int}` as an $(x, y)$ coordinate tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35e0f93f-300b-4080-a51d-924e19e7647f",
   "metadata": {},
   "outputs": [],
   "source": [
    " startstate = (20,20); # where to start the path simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b34c5a6",
   "metadata": {},
   "source": [
    "This code block generates the plot showing the optimal navigation path from the starting position to the goal while avoiding hazards. `Unhide` the cell to see the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb618c37-aa80-42f6-bb10-e277e60a9f60",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `my_œÄ` not defined in `Main`\nSuggestion: add an appropriate import or assignment. This global was declared but not assigned.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `my_œÄ` not defined in `Main`\n",
      "Suggestion: add an appropriate import or assignment. This global was declared but not assigned.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Desktop/julia_work/CHEME-5800-Instances/Fall-2025/CHEME-5800-Labs-Fall-2025/labs/week-11/L11d/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X54sZmlsZQ==.jl:18"
     ]
    }
   ],
   "source": [
    "let\n",
    "    # setup \n",
    "    world_model = world;\n",
    "    is_reward_shaping_on = false;\n",
    "   \n",
    "    # draw the path -\n",
    "    p = plot();\n",
    "    initial_site = startstate\n",
    "    hit_absorbing_state = false\n",
    "    s = world_model.states[initial_site];\n",
    "    visited_sites = Set{Tuple{Int,Int}}();\n",
    "    push!(visited_sites, initial_site);\n",
    "\n",
    "    s‚Ä≤ = s;\n",
    "    while (hit_absorbing_state == false)\n",
    "        \n",
    "        current_position = world_model.coordinates[s‚Ä≤]\n",
    "        a = my_œÄ[s‚Ä≤];\n",
    "        Œî = world_model.moves[a];\n",
    "        new_position =  current_position .+ Œî\n",
    "        scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, msc=:black, c=:blue)\n",
    "        plot!([current_position[1], new_position[1]],[current_position[2],new_position[2]], label=\"\", arrow=true, lw=1, c=:gray)\n",
    "\n",
    "        s‚Ä≤ = nothing;\n",
    "        if (in(new_position, absorbing_state_set) == true || in(new_position, visited_sites) == true)\n",
    "            hit_absorbing_state = true;\n",
    "        elseif (haskey(world_model.states, new_position) == true)\n",
    "            s‚Ä≤ = world_model.states[new_position];\n",
    "            push!(visited_sites, new_position);\n",
    "        else\n",
    "            hit_absorbing_state = true; \n",
    "        end\n",
    "    end\n",
    "\n",
    "    # draw the grid -\n",
    "    for s ‚àà ùíÆ\n",
    "        current_position = world_model.coordinates[s]\n",
    "        a = my_œÄ[s];\n",
    "        Œî = world_model.moves[a];\n",
    "        new_position =  current_position .+ Œî\n",
    "        \n",
    "         if (haskey(rewards, current_position) == true && rewards[current_position] == charging_reward)\n",
    "            scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, c=:green, ms=4)\n",
    "        elseif (haskey(rewards, current_position) == true && rewards[current_position] == lava_reward)\n",
    "            scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, c=:red, ms=4)\n",
    "        elseif (in(current_position, soft_wall_set) == true)\n",
    "            scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, c=:gray69, ms=4)\n",
    "        else\n",
    "            if (is_reward_shaping_on == true)\n",
    "                new_color = weighted_color_mean(rbf(current_position, charging_station_coordinates, œÉ = œÉ), colorant\"green\", colorant\"white\")\n",
    "                scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, msc=:black, c=new_color)\n",
    "            else\n",
    "                scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, msc=:black, c=:white)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    current()\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef04bd20",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb5e5d",
   "metadata": {},
   "source": [
    "## Discussion: Extensions and Open Questions\n",
    "Now that we have a working value iteration implementation, there are many interesting directions to explore that would deepen our understanding of MDPs and reinforcement learning:\n",
    "\n",
    "> __Future Directions:__\n",
    "> 1. **Impact of discount factor:** How does changing $\\gamma$ affect the optimal policy? Try values like $\\gamma = 0.5$ (myopic, prioritizes immediate rewards) versus $\\gamma = 0.99$ (far-sighted, values long-term rewards). Do paths change? Does the robot take different routes?\n",
    "> \n",
    "> 2. **Stochastic transitions:** Our current model has deterministic transitions where actions always succeed. What if we introduced uncertainty? For example, when the robot tries to move up, it could succeed 80% of the time but accidentally move left/right 10% each. How would this affect the optimal policy and path safety margins around lava pits?\n",
    "> \n",
    "> 3. **Convergence speed:** How many iterations does value iteration require to converge for different grid sizes? Try 5√ó5, 10√ó10, and 40√ó40 grids. How does the number of iterations scale with state space size? What about different tolerance values $\\epsilon$?\n",
    "> \n",
    "> 4. **Multiple goals:** What happens if we add multiple charging stations at different locations with different rewards? Does the robot always choose the closest one, or does the path efficiency trade-off matter?\n",
    "> \n",
    "> 5. **Continuous state spaces:** Our grid is discrete, but real robots operate in continuous space. How would you extend this approach to handle continuous positions and orientations? (Hint: look into function approximation and deep reinforcement learning)\n",
    "\n",
    "These extensions connect to important topics in reinforcement learning, robotics, and optimal control. Feel free to experiment!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d95a53",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this example, we implemented the value iteration algorithm to solve a 20√ó20 grid world navigation task, computing the optimal policy for a robot to reach a charging station while avoiding lava pits from any starting position.\n",
    "\n",
    "> __Key Takeaways:__\n",
    "> \n",
    "> * **MDP formulation for spatial navigation:** We constructed a Markov Decision Process with 400 states representing grid positions, 4 directional movement actions, deterministic transition dynamics, and a reward structure balancing goal achievement (+100 for charging station), hazard avoidance (-1000 for lava pits), and path efficiency (-1 step cost), all discounted to prioritize reaching the goal quickly.\n",
    "> * **Value iteration with Bellman updates:** We iteratively computed the optimal value function by applying the Bellman backup operation, which propagates utility values backward from high-reward states through the state space until convergence. This process evaluates the long-term value of every grid position by considering both immediate rewards and discounted future returns.\n",
    "> * **Policy extraction and visualization:** We derived the optimal policy by computing action-value functions from the converged value function and selecting the best action at each state. Visualizing the resulting navigation paths showed how the robot optimally navigates from arbitrary starting positions to the charging station while circumventing hazards, demonstrating that value iteration produces rational goal-directed behavior.\n",
    "\n",
    "Value iteration provides a principled approach to solving sequential decision problems by computing policies that maximize expected cumulative reward, with applications in robotics, autonomous systems, and resource allocation.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8deb30",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.1",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
